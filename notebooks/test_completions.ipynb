{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Text Completions: GPT-2 vs TinyGPT\n",
    "\n",
    "Load either HuggingFace GPT-2 or a TinyGPT checkpoint and run text completions.\n",
    "Supports plain text prompts and conversation-style prompts with special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaccosenza/code/project-llm-chat/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: xpu\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from core.models import TinyGPT\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'xpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set `MODEL_TYPE` to `'gpt2'` or `'tinygpt'`. For TinyGPT, point `CHECKPOINT_PATH` to a `.pt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'tinygpt'  # 'gpt2' or 'tinygpt'\n",
    "CHECKPOINT_PATH = '../logs/pu94vo4r/checkpoints/checkpoint_60000.pt'  # only used for tinygpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 50262, BOS=50257, EOS=50256\n",
      "Loaded TinyGPT from ../logs/pu94vo4r/checkpoints/checkpoint_60000.pt (525M params)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer (shared, always has special tokens)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "special_tokens = {\n",
    "    'bos_token': '<|beginoftext|>',\n",
    "    'pad_token': '<|pad|>',\n",
    "    'additional_special_tokens': ['<|user|>', '<|assistant|>', '<|system|>']\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "print(f'Vocab: {len(tokenizer)}, BOS={tokenizer.bos_token_id}, EOS={tokenizer.eos_token_id}')\n",
    "\n",
    "if MODEL_TYPE == 'gpt2':\n",
    "    model = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    print(f'Loaded GPT-2 ({sum(p.numel() for p in model.parameters())/1e6:.0f}M params)')\n",
    "\n",
    "elif MODEL_TYPE == 'tinygpt':\n",
    "    n_layers = 20\n",
    "    dim = n_layers * 64\n",
    "    n_heads = max(1, (dim + 127) // 128)\n",
    "    model = TinyGPT(vocab_size=50262, dim=dim, n_layers=n_layers, n_heads=n_heads, max_seq_len=2048)\n",
    "\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "    # Resize embeddings for special tokens\n",
    "    old_vocab = model.tok_emb.num_embeddings\n",
    "    new_vocab = len(tokenizer)\n",
    "    if old_vocab < new_vocab:\n",
    "        old_tok = model.tok_emb.weight.data.clone()\n",
    "        old_head = model.head.weight.data.clone()\n",
    "        model.tok_emb = nn.Embedding(new_vocab, dim)\n",
    "        model.head = nn.Linear(dim, new_vocab, bias=False)\n",
    "        model.tok_emb.weight.data[:old_vocab] = old_tok\n",
    "        model.head.weight.data[:old_vocab] = old_head\n",
    "        print(f'Resized embeddings: {old_vocab} -> {new_vocab}')\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f'Loaded TinyGPT from {CHECKPOINT_PATH} ({sum(p.numel() for p in model.parameters())/1e6:.0f}M params)')\n",
    "\n",
    "else:\n",
    "    raise ValueError(f'Unknown MODEL_TYPE: {MODEL_TYPE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Generation function\n",
    "\n",
    "Stops at EOS (`<|endoftext|>`) so completions don't run on forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate() ready\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prompt_ids, max_new_tokens=200, temperature=1.0, top_k=40, stop_at_eos=True):\n",
    "    \"\"\"Generate tokens from prompt_ids tensor. Stops at EOS if stop_at_eos=True.\"\"\"\n",
    "    idx = prompt_ids.to(device)\n",
    "    eos_id = tokenizer.eos_token_id if stop_at_eos else None\n",
    "    generated = []\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        if MODEL_TYPE == 'gpt2':\n",
    "            logits = model(idx).logits[:, -1, :] / temperature\n",
    "        else:\n",
    "            idx_cond = idx[:, -2048:]\n",
    "            logits = model(idx_cond)[:, -1, :] / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if eos_id is not None and next_id.item() == eos_id:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, next_id), dim=1)\n",
    "        generated.append(next_id.item())\n",
    "\n",
    "    return generated\n",
    "\n",
    "print('generate() ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Plain text completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:     The capital of France is\n",
      "COMPLETION:  Paris. It is the home of the famous Louvre Museum. Paris is also the capital of the French Republic.\n",
      "The capital of France is also Paris, the capital of the French Republic. Paris is also the capital of the United Kingdom. It\n",
      "(50 tokens, stop_at_eos=True)\n",
      "--------------------------------------------------------------------------------\n",
      "PROMPT:     In machine learning, a neural network\n",
      "COMPLETION:  is a neural network used to learn patterns using data. They are widely used across a range of industries, including healthcare, finance, and education.\n",
      "One of the most important advantages of neural networks is that they are able to learn a lot more detail\n",
      "(50 tokens, stop_at_eos=True)\n",
      "--------------------------------------------------------------------------------\n",
      "PROMPT:     The most important thing about science is\n",
      "COMPLETION:  it is there is no need for science, it does not need to be. Science can be learned from people all over the world, it is the only way we can do it. Science is a necessary part of our everyday life, but it is\n",
      "(50 tokens, stop_at_eos=True)\n",
      "--------------------------------------------------------------------------------\n",
      "PROMPT:     To make a good cup of coffee\n",
      "COMPLETION: .\n",
      "The science behind coffee isn’t new to us. It’s been around for thousands of years, and has been used to treat many ailments, from tooth loss to skin problems to headaches. Even today, it remains an important\n",
      "(50 tokens, stop_at_eos=True)\n",
      "--------------------------------------------------------------------------------\n",
      "PROMPT:     The best country in the world is\n",
      "COMPLETION:  that in the name of freedom.\n",
      "We have given you the best possible essay writing service and you can get your assignment done immediately. Get an original essay and written according to your instructions.\n",
      "We have provided you with the best possible essay writing service\n",
      "(50 tokens, stop_at_eos=True)\n",
      "--------------------------------------------------------------------------------\n",
      "PROMPT:     A doctor is best for\n",
      "COMPLETION:  you.\n",
      "How to read a blood test for tuberculosis?\n",
      "In general, a blood test for tuberculosis (TB) is needed when the TB is severe enough to require a tuberculosis drug.\n",
      "Can you drink alcohol if you have type 2 diabetes?\n",
      "(50 tokens, stop_at_eos=True)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def complete(prompt, max_new_tokens=50, temperature=0.8, top_k=40, stop_at_eos=True):\n",
    "    \"\"\"Encode prompt string, generate, decode, print.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    generated = generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                         temperature=temperature, top_k=top_k, stop_at_eos=stop_at_eos)\n",
    "    completion = tokenizer.decode(generated)\n",
    "    print(f'PROMPT:     {prompt}')\n",
    "    print(f'COMPLETION: {completion}')\n",
    "    print(f'({len(generated)} tokens, stop_at_eos={stop_at_eos})')\n",
    "    print('-' * 80)\n",
    "    return completion\n",
    "\n",
    "prompts = [\n",
    "    'The capital of France is',\n",
    "    'In machine learning, a neural network',\n",
    "    'The most important thing about science is',\n",
    "    'To make a good cup of coffee',\n",
    "    'The best country in the world is',\n",
    "    'A doctor is best for'\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    complete(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Conversation completions\n",
    "\n",
    "Use special tokens to prompt the model in chat format:\n",
    "`<|beginoftext|><|system|>...<|user|>...<|assistant|>`\n",
    "\n",
    "The model generates the assistant response and stops at `<|endoftext|>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:      What is 2 + 2?\n",
      "SYSTEM:    You are a helpful math tutor.\n",
      "ASSISTANT: We can solve this equation by combining like terms: 2 + 2 = 2.\n",
      "(16 tokens)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We can solve this equation by combining like terms: 2 + 2 = 2.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_complete(user_message, system_message=None, max_new_tokens=300, temperature=0.8, top_k=40):\n",
    "    \"\"\"Build a conversation prompt with special tokens and generate assistant response.\"\"\"\n",
    "    tokens = [tokenizer.bos_token_id]\n",
    "\n",
    "    if system_message:\n",
    "        tokens.append(tokenizer.convert_tokens_to_ids('<|system|>'))\n",
    "        tokens.extend(tokenizer.encode(system_message, add_special_tokens=False))\n",
    "\n",
    "    tokens.append(tokenizer.convert_tokens_to_ids('<|user|>'))\n",
    "    tokens.extend(tokenizer.encode(user_message, add_special_tokens=False))\n",
    "    tokens.append(tokenizer.convert_tokens_to_ids('<|assistant|>'))\n",
    "\n",
    "    input_ids = torch.tensor([tokens])\n",
    "    generated = generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                         temperature=temperature, top_k=top_k, stop_at_eos=True)\n",
    "\n",
    "    response = tokenizer.decode(generated)\n",
    "    print(f'USER:      {user_message}')\n",
    "    if system_message:\n",
    "        print(f'SYSTEM:    {system_message}')\n",
    "    print(f'ASSISTANT: {response}')\n",
    "    print(f'({len(generated)} tokens)')\n",
    "    print('-' * 80)\n",
    "    return response\n",
    "\n",
    "chat_complete('What is 2 + 2?', system_message='You are a helpful math tutor.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Multi-turn conversation\n",
    "\n",
    "Each turn appends the generated assistant response to context before the next user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTotal context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokens\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mmulti_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWhat is photosynthesis?\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWhy is it important for life on Earth?\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCan it happen without sunlight?\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mmulti_turn\u001b[39m\u001b[34m(turns, system_message, max_new_tokens, temperature, top_k)\u001b[39m\n\u001b[32m     12\u001b[39m tokens.append(tokenizer.convert_tokens_to_ids(\u001b[33m'\u001b[39m\u001b[33m<|assistant|>\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     14\u001b[39m input_ids = torch.tensor([tokens])\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m generated = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_at_eos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m response = tokenizer.decode(generated)\n\u001b[32m     19\u001b[39m tokens.extend(generated)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/project-llm-chat/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(prompt_ids, max_new_tokens, temperature, top_k, stop_at_eos)\u001b[39m\n\u001b[32m     16\u001b[39m     v, _ = torch.topk(logits, \u001b[38;5;28mmin\u001b[39m(top_k, logits.size(-\u001b[32m1\u001b[39m)))\n\u001b[32m     17\u001b[39m     logits[logits < v[:, [-\u001b[32m1\u001b[39m]]] = -\u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mInf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m probs = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m next_id = torch.multinomial(probs, num_samples=\u001b[32m1\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eos_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m next_id.item() == eos_id:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/project-llm-chat/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2120\u001b[39m, in \u001b[36msoftmax\u001b[39m\u001b[34m(input, dim, _stacklevel, dtype)\u001b[39m\n\u001b[32m   2116\u001b[39m         ret = (-\u001b[38;5;28minput\u001b[39m).softmax(dim, dtype=dtype)\n\u001b[32m   2117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[32m-> \u001b[39m\u001b[32m2120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msoftmax\u001b[39m(\n\u001b[32m   2121\u001b[39m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[32m   2122\u001b[39m     dim: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2123\u001b[39m     _stacklevel: \u001b[38;5;28mint\u001b[39m = \u001b[32m3\u001b[39m,\n\u001b[32m   2124\u001b[39m     dtype: Optional[DType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2125\u001b[39m ) -> Tensor:\n\u001b[32m   2126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Apply a softmax function.\u001b[39;00m\n\u001b[32m   2127\u001b[39m \n\u001b[32m   2128\u001b[39m \u001b[33;03m    Softmax is defined as:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2148\u001b[39m \n\u001b[32m   2149\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def multi_turn(turns, system_message=None, max_new_tokens=300, temperature=0.8, top_k=40):\n",
    "    \"\"\"Multi-turn conversation. `turns` is a list of user messages.\"\"\"\n",
    "    tokens = [tokenizer.bos_token_id]\n",
    "\n",
    "    if system_message:\n",
    "        tokens.append(tokenizer.convert_tokens_to_ids('<|system|>'))\n",
    "        tokens.extend(tokenizer.encode(system_message, add_special_tokens=False))\n",
    "\n",
    "    for i, user_msg in enumerate(turns):\n",
    "        tokens.append(tokenizer.convert_tokens_to_ids('<|user|>'))\n",
    "        tokens.extend(tokenizer.encode(user_msg, add_special_tokens=False))\n",
    "        tokens.append(tokenizer.convert_tokens_to_ids('<|assistant|>'))\n",
    "\n",
    "        input_ids = torch.tensor([tokens])\n",
    "        generated = generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                             temperature=temperature, top_k=top_k, stop_at_eos=True)\n",
    "\n",
    "        response = tokenizer.decode(generated)\n",
    "        tokens.extend(generated)\n",
    "        tokens.append(tokenizer.eos_token_id)  # close this turn\n",
    "\n",
    "        print(f'[Turn {i+1}]')\n",
    "        print(f'  USER:      {user_msg}')\n",
    "        print(f'  ASSISTANT: {response}')\n",
    "        print()\n",
    "\n",
    "    print(f'Total context: {len(tokens)} tokens')\n",
    "    print('-' * 80)\n",
    "\n",
    "multi_turn([\n",
    "    'What is photosynthesis?',\n",
    "    'Why is it important for life on Earth?',\n",
    "    'Can it happen without sunlight?',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Temperature comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: The meaning of life is\n",
      "\n",
      "  T=0.3:  to be found in the life of the human being. The life of the human being is the life of the universe. Life is the life of the universe. Life is the life of the universe. Life is the life of the univer\n",
      "  T=0.7:  not known.\n",
      "The word \"life\" is used to describe something in all its complexity. The word \"life\" is an abstract idea that has been used in various cultures. It is also used in the context of the conce\n",
      "  T=1.0:  more important as it relates to our own lives and is in harmony with the world around us.\n",
      "If we look at each other in the mirror, we look at ourselves and look at each other’s lives. Without self dis\n",
      "--------------------------------------------------------------------------------\n",
      "PROMPT: In the year 2050\n",
      "\n",
      "  T=0.3: , the world’s population will increase by more than 7 billion people, and the world’s population will increase by more than 9 billion people.\n",
      "The number of people in the world is expected to rise by m\n",
      "  T=0.7: , the world’s population is estimated to grow to 7.6 billion. What is predicted to happen by 2050 is an estimated 9.2 billion people living in extreme poverty, an estimated 8.4 billion people in extre\n",
      "  T=1.0: , we should go down in the history books and remember the sacrifices of the past. These sacrifices not only made a difference today, but they will continue in the future.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def compare_temps(prompt, temps=[0.3, 0.7, 1.0], max_new_tokens=100):\n",
    "    print(f'PROMPT: {prompt}\\n')\n",
    "    for t in temps:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        generated = generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                             temperature=t, top_k=40, stop_at_eos=True)\n",
    "        text = tokenizer.decode(generated)[:200]\n",
    "        print(f'  T={t}: {text}')\n",
    "    print('-' * 80)\n",
    "\n",
    "compare_temps('The meaning of life is')\n",
    "compare_temps('In the year 2050')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Greedy decoding (deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: The capital of France is\n",
      "GREEDY:  Paris. Paris is the capital of France. Paris is the capital of France. Paris is the capital of France. Paris is the capital of France. Paris is the capital of France. Paris is the capital of France. Paris is the capital of France\n",
      "--------------------------------------------------------------------------------\n",
      "PROMPT: 2 + 2 =\n",
      "GREEDY:  4\n",
      "- 2 + 2 = 6\n",
      "- 2 + 2 = 8\n",
      "- 2 + 2 = 9\n",
      "- 2 + 2 = 10\n",
      "- 2 + 2 = 11\n",
      "- 2 + 2 = 12\n",
      "- 2 + 2 = 13\n",
      "--------------------------------------------------------------------------------\n",
      "PROMPT: The largest planet in our solar system is\n",
      "GREEDY:  Mercury. Mercury is the second largest planet in our solar system after Earth. Mercury is the second largest planet in our solar system after Earth. Mercury is the second largest planet in our solar system after Earth. Mercury is the second largest planet in our solar\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def greedy_complete(prompt, max_new_tokens=50):\n",
    "    \"\"\"Deterministic greedy decoding (temperature ~0).\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    generated = generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                         temperature=0.01, top_k=1, stop_at_eos=True)\n",
    "    text = tokenizer.decode(generated)\n",
    "    print(f'PROMPT: {prompt}')\n",
    "    print(f'GREEDY: {text}')\n",
    "    print('-' * 80)\n",
    "\n",
    "greedy_complete('The capital of France is')\n",
    "greedy_complete('2 + 2 =')\n",
    "greedy_complete('The largest planet in our solar system is')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
