{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Completions\n",
    "\n",
    "Download GPT-2 weights and run text completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaccosenza/code/project-llm-dnd/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Save GPT-2 Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights will be saved to: /home/zaccosenza/code/project-llm-chat/weights/gpt2\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "weights_dir = Path(\"../weights/gpt2\")\n",
    "weights_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Weights will be saved to: {weights_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 289.71it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Download GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # Options: gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "\n",
    "print(f\"Downloading {model_name}...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to ../weights/gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [-00:00<00:00, -1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n",
      "Contents: [PosixPath('../weights/gpt2/config.json'), PosixPath('../weights/gpt2/tokenizer_config.json'), PosixPath('../weights/gpt2/generation_config.json'), PosixPath('../weights/gpt2/model.safetensors'), PosixPath('../weights/gpt2/tokenizer.json')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save weights locally\n",
    "print(f\"Saving to {weights_dir}...\")\n",
    "model.save_pretrained(weights_dir)\n",
    "tokenizer.save_pretrained(weights_dir)\n",
    "\n",
    "print(\"Saved!\")\n",
    "print(f\"Contents: {list(weights_dir.iterdir())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 480.40it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load from local weights\n",
    "device = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(weights_dir).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(weights_dir)\n",
    "\n",
    "# Set pad token (GPT-2 doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Completion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    num_return_sequences: int = 1,\n",
    ") -> list[str]:\n",
    "    \"\"\"Generate text completions for a given prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    completions = []\n",
    "    for output in outputs:\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        completions.append(text)\n",
    "    \n",
    "    return completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"In a shocking turn of events, scientists discovered that\",\n",
    "    \"The best way to learn programming is\",\n",
    "    \"Once upon a time, in a land far away,\",\n",
    "    \"The meaning of life is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Prompt: The capital of France is\n",
      "============================================================\n",
      "\n",
      "Completion 1:\n",
      "The capital of France is the most populous city in Europe and is home to more than 4,000,000 inhabitants. It is home to the oldest and largest military base in the world, the French Air Force. The capital has also been the site of the biggest concentration of\n",
      "\n",
      "============================================================\n",
      "Prompt: In a shocking turn of events, scientists discovered that\n",
      "============================================================\n",
      "\n",
      "Completion 1:\n",
      "In a shocking turn of events, scientists discovered that the sun has been changing its orbit around the planet for two years.\n",
      "\n",
      "The results, published online today in the journal Nature Geoscience, also reveal that the planet has been moving toward its closest approach to the sun. The planet is currently\n",
      "\n",
      "============================================================\n",
      "Prompt: The best way to learn programming is\n",
      "============================================================\n",
      "\n",
      "Completion 1:\n",
      "The best way to learn programming is to be familiar with the programming language and how to use it.\n",
      "\n",
      "To start, get started. If you're familiar with programming, then get a programming degree and start learning programming. This way you'll be able to start learning programming in a\n",
      "\n",
      "============================================================\n",
      "Prompt: Once upon a time, in a land far away,\n",
      "============================================================\n",
      "\n",
      "Completion 1:\n",
      "Once upon a time, in a land far away, there was a man who was the father of the daughter of the king of the kings, who was called the daughter of the daughter of the king of the kings, but the son of the son of the king of the kings, that was the name\n",
      "\n",
      "============================================================\n",
      "Prompt: The meaning of life is\n",
      "============================================================\n",
      "\n",
      "Completion 1:\n",
      "The meaning of life is our relationship to our partner. We do not have to live with them, we don't have to be in a relationship with them. We can take care of our own lives. And we can help those around us.\n",
      "\n",
      "Our lives are not\n"
     ]
    }
   ],
   "source": [
    "# Generate completions\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    completions = generate_completion(prompt, max_new_tokens=50, temperature=0.8)\n",
    "    \n",
    "    for i, completion in enumerate(completions):\n",
    "        print(f\"\\nCompletion {i+1}:\")\n",
    "        print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Decoding (Temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy(prompt: str, max_new_tokens: int = 50) -> str:\n",
    "    \"\"\"Generate text using greedy decoding (deterministic).\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: The capital of France is\n",
      "Output: The capital of France is the capital of the French Republic, and the capital of the French Republic is the capital of the French Republic.\n",
      "\n",
      "The French Republic is the capital\n",
      "\n",
      "Prompt: 2 + 2 =\n",
      "Output: 2 + 2 = 3.5 + 3.5 + 3.5 + 3.5 + 3.5 + 3.5 + 3.5 + 3.\n",
      "\n",
      "Prompt: The largest planet in our solar system is\n",
      "Output: The largest planet in our solar system is about 1.5 billion light years away.\n",
      "\n",
      "The planet is about 1.5 billion light years from Earth.\n",
      "\n",
      "The planet is about\n"
     ]
    }
   ],
   "source": [
    "# Test greedy decoding\n",
    "test_prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"2 + 2 =\",\n",
    "    \"The largest planet in our solar system is\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {generate_greedy(prompt, max_new_tokens=30)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The future of artificial intelligence is\n",
      "\n",
      "Completion:\n",
      "The future of artificial intelligence is bright. As we learn more about the future of AI, it's likely that we'll find that AI is not only smarter, but also smarter than it was in the past.\n",
      "\n",
      "The Future of Artificial Intelligence\n",
      "\n",
      "AI is changing how we think about life, how we interact with our environment, and how we communicate with other people.\n",
      "\n",
      "While we might not see it all, there are some things we can learn from the past that are still relevant today.\n",
      "\n",
      "The first thing\n"
     ]
    }
   ],
   "source": [
    "# Try your own prompt\n",
    "custom_prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(f\"Prompt: {custom_prompt}\\n\")\n",
    "print(\"Completion:\")\n",
    "print(generate_completion(custom_prompt, max_new_tokens=100, temperature=0.7)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
