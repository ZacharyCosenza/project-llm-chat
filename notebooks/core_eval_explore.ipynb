{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE Eval Data Exploration\n",
    "\n",
    "Explore the CORE benchmark eval bundle (from the DCLM paper).\n",
    "Run `python -m core.dataset --eval` first to download the bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaccosenza/code/project-llm-chat/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from jinja2 import Template\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "EVAL_DIR = Path(\"../data/eval_data\")\n",
    "assert EVAL_DIR.exists(), \"Run 'python -m core.dataset --eval' first\"\n",
    "\n",
    "with open(EVAL_DIR / \"core.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "tasks = config[\"icl_tasks\"]\n",
    "\n",
    "def load_data(task):\n",
    "    with open(EVAL_DIR / \"eval_data\" / task[\"dataset_uri\"]) as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Overview: 0-shot vs Few-shot\n",
    "\n",
    "Each CORE task specifies `num_fewshot` â€” how many examples from the same dataset are\n",
    "prepended as demonstrations before the actual query. The split is:\n",
    "- **0-shot**: model sees only the query, no demonstrations\n",
    "- **Few-shot (3 or 10)**: model sees N solved examples before the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                                  Type                 N-shot   Delimiter\n",
      "-----------------------------------------------------------------------------------------------\n",
      "hellaswag_zeroshot                     multiple_choice      0-shot   ' '\n",
      "jeopardy                               language_modeling    10-shot  '\\nAnswer: '\n",
      "bigbench_qa_wikidata                   language_modeling    10-shot  ' '\n",
      "arc_easy                               multiple_choice      10-shot  '\\nAnswer: '\n",
      "arc_challenge                          multiple_choice      10-shot  '\\nAnswer: '\n",
      "copa                                   multiple_choice      0-shot   ' '\n",
      "commonsense_qa                         multiple_choice      10-shot  ' '\n",
      "piqa                                   multiple_choice      10-shot  '\\nAnswer: '\n",
      "openbook_qa                            multiple_choice      0-shot   ' '\n",
      "lambada_openai                         language_modeling    0-shot   ' '\n",
      "hellaswag                              multiple_choice      10-shot  ' '\n",
      "winograd                               schema               0-shot   ' '\n",
      "winogrande                             schema               0-shot   ' '\n",
      "bigbench_dyck_languages                language_modeling    10-shot  ' '\n",
      "agi_eval_lsat_ar                       multiple_choice      3-shot   ' '\n",
      "bigbench_cs_algorithms                 language_modeling    10-shot  ' '\n",
      "bigbench_operators                     language_modeling    10-shot  ' '\n",
      "bigbench_repeat_copy_logic             language_modeling    10-shot  ' '\n",
      "squad                                  language_modeling    10-shot  ' '\n",
      "coqa                                   language_modeling    0-shot   ' '\n",
      "boolq                                  multiple_choice      10-shot  '\\nAnswer: '\n",
      "bigbench_language_identification       multiple_choice      10-shot  ' '\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Label':<38} {'Type':<20} {'N-shot':<8} {'Delimiter'}\")\n",
    "print(\"-\" * 95)\n",
    "for t in tasks:\n",
    "    n = t['num_fewshot'][0]\n",
    "    d = repr(t.get('continuation_delimiter', ' '))\n",
    "    shot_label = f\"{n}-shot\"\n",
    "    print(f\"{t['label']:<38} {t['icl_task_type']:<20} {shot_label:<8} {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-shot Multiple Choice: hellaswag_zeroshot\n",
    "\n",
    "No demonstrations. The model only sees the query + each candidate answer.\n",
    "The prompt sent to the model for each choice is just:\n",
    "```\n",
    "<query> <choice>\n",
    "```\n",
    "The model scores each by computing mean cross-entropy over the answer tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: hellaswag_zeroshot (0-shot)\n",
      "Type: multiple_choice\n",
      "Query: Roof shingle removal: A man is sitting on a roof. He\n",
      "Gold: choice 3 = 'starts pulling up roofing on a roof.'\n",
      "\n",
      "--- Choice 0 ---\n",
      "Full prompt (26 tokens):\n",
      "Roof shingle removal: A man is sitting on a roof. He is using wrap to wrap a pair of skis.\n",
      "\n",
      "--- Choice 1 ---\n",
      "Full prompt (21 tokens):\n",
      "Roof shingle removal: A man is sitting on a roof. He is ripping level tiles off.\n",
      "\n",
      "--- Choice 2 ---\n",
      "Full prompt (23 tokens):\n",
      "Roof shingle removal: A man is sitting on a roof. He is holding a rubik's cube.\n",
      "\n",
      "--- Choice 3 <-- GOLD ---\n",
      "Full prompt (24 tokens):\n",
      "Roof shingle removal: A man is sitting on a roof. He starts pulling up roofing on a roof.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task_0shot = next(t for t in tasks if t['label'] == 'hellaswag_zeroshot')\n",
    "data_0shot = load_data(task_0shot)\n",
    "ex = data_0shot[0]\n",
    "delimiter = task_0shot.get('continuation_delimiter', ' ')\n",
    "\n",
    "print(f\"Task: {task_0shot['label']} ({task_0shot['num_fewshot'][0]}-shot)\")\n",
    "print(f\"Type: {task_0shot['icl_task_type']}\")\n",
    "print(f\"Query: {ex['query']}\")\n",
    "print(f\"Gold: choice {ex['gold']} = {ex['choices'][ex['gold']]!r}\")\n",
    "print()\n",
    "\n",
    "# Show the exact prompt the model sees for each choice\n",
    "for i, choice in enumerate(ex['choices']):\n",
    "    prompt = f\"{ex['query']}{delimiter}{choice}\"\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    gold = \" <-- GOLD\" if i == ex['gold'] else \"\"\n",
    "    print(f\"--- Choice {i}{gold} ---\")\n",
    "    print(f\"Full prompt ({len(tokens)} tokens):\")\n",
    "    print(prompt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-shot Multiple Choice: arc_easy\n",
    "\n",
    "10 solved examples are prepended. The model sees:\n",
    "```\n",
    "<query_1>\\nAnswer: <gold_answer_1>\n",
    "\n",
    "<query_2>\\nAnswer: <gold_answer_2>\n",
    "\n",
    "... (10 total)\n",
    "\n",
    "<actual_query>\\nAnswer: <candidate_choice>\n",
    "```\n",
    "The few-shot examples teach the model the task format via in-context learning.\n",
    "Loss is only measured over the candidate choice tokens (after the common prefix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: arc_easy (10-shot)\n",
      "Type: multiple_choice\n",
      "Delimiter: '\\nAnswer: '\n",
      "Query: Question: Which statement best explains why photosynthesis is the foundation of most food webs?\n",
      "Gold: choice 0 = 'Sunlight is the source of energy for nearly all ecosystems.'\n",
      "\n",
      "=== Full 10-shot prompt for GOLD choice (338 tokens) ===\n",
      "Question: The first telescopes were invented hundreds of years ago. Which was discovered as a result of this invention?\n",
      "Answer: the moons of Jupiter\n",
      "\n",
      "Question: Some birds fly south in the fall and return in the spring. This is an example of\n",
      "Answer: migration\n",
      "\n",
      "Question: In order for cells to grow at a normal rate, they must\n",
      "Answer: take in nutrients.\n",
      "\n",
      "Question: Soil is important to a forest because the soil ___.\n",
      "Answer: provides nutrients to the trees\n",
      "\n",
      "Question: Leather basketballs are made for indoor use on smooth surfaces. Rubber basketballs are made for use on many different surfaces. Which of the following properties of rubber makes it better than leather for use on many different surfaces?\n",
      "Answer: Rubber is durable.\n",
      "\n",
      "Question: A stationary air mass over the Gulf of Mexico would typically be\n",
      "Answer: warm and humid.\n",
      "\n",
      "Question: Which of the following is an example of a compound?\n",
      "Answer: water\n",
      "\n",
      "Question: A biological community is made up of all the\n",
      "Answer: natural resources present in an area\n",
      "\n",
      "Question: Five hundred flies of one species were sprayed with a new insecticide. Twenty-four hours later, nearly all the flies were dead. However, a few survived. Which principle best explains why some flies survived?\n",
      "Answer: There is variation among individuals within a species.\n",
      "\n",
      "Question: Erin wants to make a strong, light chair. Which natural material should she use?\n",
      "Answer: wood\n",
      "\n",
      "Question: Which statement best explains why photosynthesis is the foundation of most food webs?\n",
      "Answer: Sunlight is the source of energy for nearly all ecosystems.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task_10shot = next(t for t in tasks if t['label'] == 'arc_easy')\n",
    "data_10shot = load_data(task_10shot)\n",
    "delimiter = task_10shot.get('continuation_delimiter', ' ')\n",
    "num_fewshot = task_10shot['num_fewshot'][0]\n",
    "\n",
    "# Pick example and sample few-shot demos (same logic as core_eval.py)\n",
    "idx = 0\n",
    "ex = data_10shot[idx]\n",
    "rng = random.Random(1234 + idx)\n",
    "available = [i for i in range(len(data_10shot)) if i != idx]\n",
    "fewshot = [data_10shot[i] for i in rng.sample(available, num_fewshot)]\n",
    "\n",
    "print(f\"Task: {task_10shot['label']} ({num_fewshot}-shot)\")\n",
    "print(f\"Type: {task_10shot['icl_task_type']}\")\n",
    "print(f\"Delimiter: {delimiter!r}\")\n",
    "print(f\"Query: {ex['query']}\")\n",
    "print(f\"Gold: choice {ex['gold']} = {ex['choices'][ex['gold']]!r}\")\n",
    "print()\n",
    "\n",
    "# Render using the same jinja2 template as core_eval.py\n",
    "tpl = Template(\n",
    "    \"{%- for ex in fewshot -%}{{ ex.query }}{{ d }}{{ ex.choices[ex.gold] }}\\n\\n\"\n",
    "    \"{% endfor -%}{{ item.query }}{{ d }}{{ choice }}\"\n",
    ")\n",
    "\n",
    "# Show the full prompt for the gold choice\n",
    "gold_prompt = tpl.render(fewshot=fewshot, d=delimiter, item=ex, choice=ex['choices'][ex['gold']])\n",
    "gold_tokens = tokenizer.encode(gold_prompt, add_special_tokens=False)\n",
    "\n",
    "print(f\"=== Full 10-shot prompt for GOLD choice ({len(gold_tokens)} tokens) ===\")\n",
    "print(gold_prompt)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common prefix: 326 tokens (few-shot demos + query + delimiter)\n",
      "This is the same across all 4 choices.\n",
      "Loss is measured ONLY on tokens after position 326.\n",
      "\n",
      "Choice 0 <-- GOLD: ' Sunlight is the source of energy for nearly all ecosystems.' (12 tokens scored)\n",
      "Choice 1: ' Most ecosystems are found on land instead of in water.' (11 tokens scored)\n",
      "Choice 2: ' Carbon dioxide is more available than other gases.' (9 tokens scored)\n",
      "Choice 3: ' The producers in all ecosystems are plants.' (8 tokens scored)\n"
     ]
    }
   ],
   "source": [
    "# Show where the scoring region is\n",
    "# All choices share the same prefix (few-shot demos + query), only the answer differs\n",
    "all_prompts = [tpl.render(fewshot=fewshot, d=delimiter, item=ex, choice=c) for c in ex['choices']]\n",
    "all_tokens = [tokenizer.encode(p, add_special_tokens=False) for p in all_prompts]\n",
    "\n",
    "# Find common prefix length\n",
    "min_len = min(len(t) for t in all_tokens)\n",
    "prefix_len = 0\n",
    "for i in range(min_len):\n",
    "    if all(t[i] == all_tokens[0][i] for t in all_tokens):\n",
    "        prefix_len = i + 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"Common prefix: {prefix_len} tokens (few-shot demos + query + delimiter)\")\n",
    "print(f\"This is the same across all {len(ex['choices'])} choices.\")\n",
    "print(f\"Loss is measured ONLY on tokens after position {prefix_len}.\")\n",
    "print()\n",
    "\n",
    "for i, (prompt, tokens) in enumerate(zip(all_prompts, all_tokens)):\n",
    "    answer_tokens = tokens[prefix_len:]\n",
    "    answer_text = tokenizer.decode(answer_tokens)\n",
    "    gold = \" <-- GOLD\" if i == ex['gold'] else \"\"\n",
    "    print(f\"Choice {i}{gold}: {answer_text!r} ({len(answer_tokens)} tokens scored)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-shot Language Modeling: lambada_openai\n",
    "\n",
    "No demonstrations. The model sees a passage and must predict the final word.\n",
    "Scoring: argmax prediction at every token position in the continuation must\n",
    "exactly match the ground truth. Binary correct/incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: bigbench_dyck_languages (10-shot)\n",
      "Type: language_modeling\n",
      "\n",
      "Context (73 tokens):\n",
      "Complete the rest of the sequence, making sure that the parentheses are closed properly. \n",
      "\n",
      "Input: [ < < { } > [ { [ ] ( ( ( ( < ( ( ) ) > ) ) ) [ ] ) } ] { } < [ { ( { < ( ) > } ) } ( ) ] > > {\n",
      "Output:\n",
      "\n",
      "Continuation: '} ]'\n",
      "Continuation tokens (1): [2361]\n",
      "Decoded: ' ]'\n",
      "\n",
      "Model must predict EVERY continuation token correctly via argmax.\n"
     ]
    }
   ],
   "source": [
    "task_lm0 = next(t for t in tasks if t['label'] == 'bigbench_dyck_languages')\n",
    "data_lm0 = load_data(task_lm0)\n",
    "delimiter = task_lm0.get('continuation_delimiter', ' ')\n",
    "ex = data_lm0[0]\n",
    "\n",
    "print(f\"Task: {task_lm0['label']} ({task_lm0['num_fewshot'][0]}-shot)\")\n",
    "print(f\"Type: {task_lm0['icl_task_type']}\")\n",
    "print()\n",
    "\n",
    "# The prompt WITHOUT continuation (what the model conditions on)\n",
    "prompt_without = ex['context'].strip() + delimiter\n",
    "# The full prompt WITH continuation (what gets forwarded)\n",
    "prompt_with = ex['context'].strip() + delimiter + ex['continuation']\n",
    "\n",
    "tokens_without = tokenizer.encode(prompt_without, add_special_tokens=False)\n",
    "tokens_with = tokenizer.encode(prompt_with, add_special_tokens=False)\n",
    "\n",
    "continuation_tokens = tokens_with[len(tokens_without):]\n",
    "\n",
    "print(f\"Context ({len(tokens_without)} tokens):\")\n",
    "print(ex['context'][:300])\n",
    "print()\n",
    "print(f\"Continuation: {ex['continuation']!r}\")\n",
    "print(f\"Continuation tokens ({len(continuation_tokens)}): {continuation_tokens}\")\n",
    "print(f\"Decoded: {tokenizer.decode(continuation_tokens)!r}\")\n",
    "print()\n",
    "print(\"Model must predict EVERY continuation token correctly via argmax.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-shot Language Modeling: jeopardy\n",
    "\n",
    "10 solved examples are prepended. The model sees:\n",
    "```\n",
    "<context_1>\\nAnswer: <answer_1>\n",
    "\n",
    "<context_2>\\nAnswer: <answer_2>\n",
    "\n",
    "... (10 total)\n",
    "\n",
    "<actual_context>\\nAnswer: <expected_answer>\n",
    "```\n",
    "The few-shot examples teach the model the Q&A format.\n",
    "Scoring is still exact argmax match on the answer tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: jeopardy (10-shot)\n",
      "Delimiter: '\\nAnswer: '\n",
      "Expected answer: 'Admiral Richard Byrd'\n",
      "\n",
      "Total prompt: 344 tokens\n",
      "Context (demos + query): 341 tokens\n",
      "Answer tokens to predict: 3 tokens\n",
      "\n",
      "=== Full 10-shot prompt ===\n",
      "WORD ORIGINS: This word for a cantankerous personality is a variation of ordinary\n",
      "Answer: ornery\n",
      "\n",
      "AMERICAN HISTORY: In 1888 he won the presidency using the campaign song Grandfathers Hat Fits Ben\n",
      "Answer: Benjamin Harrison\n",
      "\n",
      "WORLD HISTORY: In 1199 this crusader king of England was mortally wounded while besieging the castle of Chalus\n",
      "Answer: Richard the Lionhearted\n",
      "\n",
      "WORLD HISTORY: Jerusalem was captured by this king of Babylon in 597 & 586 B.C.\n",
      "Answer: Nebuchadnezzar\n",
      "\n",
      "WORLD HISTORY: In 1606, Willem Janszoon landed on Cape York Peninsula, becoming the 1st European to visit this continent\n",
      "Answer: Australia\n",
      "\n",
      "WORLD HISTORY: According to legend, he was a swineherd before he conquered the Incas\n",
      "Answer: Francisco Pizarro\n",
      "\n",
      "AMERICAN HISTORY: Victor Marie du Pont served as a captain of this states volunteers in the War of 1812\n",
      "Answer: Delaware\n",
      "\n",
      "SCIENCE: Newton figured the Earth should be an oblate spheroid with an equatorial bulge because it does this\n",
      "Answer: Spin on its Axis\n",
      "\n",
      "LITERATURE: The prep school classic A Separate Peace takes place during this war\n",
      "Answer: the Second World War\n",
      "\n",
      "WORLD HISTORY: On Feb. 18, 2008 the U.S. recognized this new Balkan state\n",
      "Answer: Kosovo\n",
      "\n",
      "WORLD HISTORY: This Navy commander flew from a base at Little America to the South Pole & back Nov. 28-29, 1929\n",
      "Answer: Admiral Richard Byrd\n",
      "\n",
      "--- Scoring region ---\n",
      "Continuation: ' Admiral Richard Byrd'\n",
      "Token IDs: [24646, 6219, 44972]\n",
      "Model must get ALL of these exactly right via argmax.\n"
     ]
    }
   ],
   "source": [
    "task_lm10 = next(t for t in tasks if t['label'] == 'jeopardy')\n",
    "data_lm10 = load_data(task_lm10)\n",
    "delimiter = task_lm10.get('continuation_delimiter', ' ')\n",
    "num_fewshot = task_lm10['num_fewshot'][0]\n",
    "\n",
    "idx = 0\n",
    "ex = data_lm10[idx]\n",
    "rng = random.Random(1234 + idx)\n",
    "available = [i for i in range(len(data_lm10)) if i != idx]\n",
    "fewshot = [data_lm10[i] for i in rng.sample(available, num_fewshot)]\n",
    "\n",
    "# Render using same templates as core_eval.py\n",
    "base_tpl = Template(\n",
    "    \"{%- for ex in fewshot -%}{{ ex.context | trim }}{{ d }}{{ ex.continuation }}\\n\\n\"\n",
    "    \"{% endfor -%}{{ item.context | trim }}{{ d }}\"\n",
    ")\n",
    "full_tpl = Template(\n",
    "    \"{%- for ex in fewshot -%}{{ ex.context | trim }}{{ d }}{{ ex.continuation }}\\n\\n\"\n",
    "    \"{% endfor -%}{{ item.context | trim }}{{ d }}{{ item.continuation }}\"\n",
    ")\n",
    "\n",
    "ctx = dict(fewshot=fewshot, d=delimiter, item=ex)\n",
    "prompt_without = base_tpl.render(**ctx).strip()\n",
    "prompt_with = full_tpl.render(**ctx)\n",
    "\n",
    "tokens_without = tokenizer.encode(prompt_without, add_special_tokens=False)\n",
    "tokens_with = tokenizer.encode(prompt_with, add_special_tokens=False)\n",
    "continuation_tokens = tokens_with[len(tokens_without):]\n",
    "\n",
    "print(f\"Task: {task_lm10['label']} ({num_fewshot}-shot)\")\n",
    "print(f\"Delimiter: {delimiter!r}\")\n",
    "print(f\"Expected answer: {ex['continuation']!r}\")\n",
    "print(f\"\\nTotal prompt: {len(tokens_with)} tokens\")\n",
    "print(f\"Context (demos + query): {len(tokens_without)} tokens\")\n",
    "print(f\"Answer tokens to predict: {len(continuation_tokens)} tokens\")\n",
    "print()\n",
    "\n",
    "print(\"=== Full 10-shot prompt ===\")\n",
    "print(prompt_with)\n",
    "print()\n",
    "\n",
    "print(f\"--- Scoring region ---\")\n",
    "print(f\"Continuation: {tokenizer.decode(continuation_tokens)!r}\")\n",
    "print(f\"Token IDs: {continuation_tokens}\")\n",
    "print(f\"Model must get ALL of these exactly right via argmax.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-shot Schema: winograd\n",
    "\n",
    "No demonstrations. Two different context options, same continuation.\n",
    "The model must judge which context makes the continuation more likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: winograd (0-shot)\n",
      "Type: schema\n",
      "Continuation: 'feared violence.'\n",
      "Gold: context option 0\n",
      "\n",
      "--- Context option 0 <-- GOLD ---\n",
      "Prompt (17 tokens): The city councilmen refused the demonstrators a permit because the city councilmen feared violence.\n",
      "\n",
      "--- Context option 1 ---\n",
      "Prompt (15 tokens): The city councilmen refused the demonstrators a permit because the demonstrators feared violence.\n",
      "\n",
      "Common suffix: 3 tokens = ' feared violence.'\n",
      "Loss is measured over this suffix for each context option.\n",
      "Lowest mean loss = model's preferred context.\n"
     ]
    }
   ],
   "source": [
    "task_schema = next(t for t in tasks if t['label'] == 'winograd')\n",
    "data_schema = load_data(task_schema)\n",
    "delimiter = task_schema.get('continuation_delimiter', ' ')\n",
    "ex = data_schema[0]\n",
    "\n",
    "print(f\"Task: {task_schema['label']} ({task_schema['num_fewshot'][0]}-shot)\")\n",
    "print(f\"Type: {task_schema['icl_task_type']}\")\n",
    "print(f\"Continuation: {ex['continuation']!r}\")\n",
    "print(f\"Gold: context option {ex['gold']}\")\n",
    "print()\n",
    "\n",
    "for i, ctx_opt in enumerate(ex['context_options']):\n",
    "    prompt = f\"{ctx_opt}{delimiter}{ex['continuation']}\"\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    gold = \" <-- GOLD\" if i == ex['gold'] else \"\"\n",
    "    print(f\"--- Context option {i}{gold} ---\")\n",
    "    print(f\"Prompt ({len(tokens)} tokens): {prompt}\")\n",
    "    print()\n",
    "\n",
    "# Show the scoring region (common suffix)\n",
    "all_prompts = [f\"{co}{delimiter}{ex['continuation']}\" for co in ex['context_options']]\n",
    "all_tokens = [tokenizer.encode(p, add_special_tokens=False) for p in all_prompts]\n",
    "\n",
    "min_len = min(len(t) for t in all_tokens)\n",
    "suffix_len = 0\n",
    "for i in range(1, min_len + 1):\n",
    "    if all(t[-i] == all_tokens[0][-i] for t in all_tokens):\n",
    "        suffix_len = i\n",
    "    else:\n",
    "        break\n",
    "\n",
    "suffix_tokens = all_tokens[0][-suffix_len:]\n",
    "print(f\"Common suffix: {suffix_len} tokens = {tokenizer.decode(suffix_tokens)!r}\")\n",
    "print(f\"Loss is measured over this suffix for each context option.\")\n",
    "print(f\"Lowest mean loss = model's preferred context.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
