{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packing Strategy Demo — Real Conversation Data\n",
    "\n",
    "Loads real SmolTalk conversations from `data/conversation_data/` and shows what `_yield_packed` vs `_yield_bos_aligned` actually yield.\n",
    "\n",
    "Run from the project root or ensure `data/conversation_data/` is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaccosenza/code/project-llm-chat/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab : 50262\n",
      "Conversation shards found : 31\n",
      "Using for demo  : ['shard_00000.parquet']\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from run_11_sft import PackedStreamingDataset\n",
    "\n",
    "# Tokenizer setup — identical to run_11_sft.py main\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '<|beginoftext|>',\n",
    "    'pad_token': '<|pad|>',\n",
    "    'additional_special_tokens': ['<|user|>', '<|assistant|>', '<|system|>']\n",
    "})\n",
    "\n",
    "bos_id  = tokenizer.bos_token_id\n",
    "eos_id  = tokenizer.eos_token_id\n",
    "pad_id  = tokenizer.pad_token_id\n",
    "usr_id  = tokenizer.convert_tokens_to_ids('<|user|>')\n",
    "asst_id = tokenizer.convert_tokens_to_ids('<|assistant|>')\n",
    "sys_id  = tokenizer.convert_tokens_to_ids('<|system|>')\n",
    "ROLE_NAMES = {bos_id: 'BOS', eos_id: 'EOS', pad_id: 'PAD',\n",
    "              usr_id: 'USR', asst_id: 'ASST', sys_id: 'SYS'}\n",
    "BOUNDARY_IDS = set(ROLE_NAMES)\n",
    "\n",
    "# Point to first conversation shard (validation shard)\n",
    "conv_dir   = Path('../data/conversation_data')\n",
    "conv_files = sorted(conv_dir.glob('*.parquet'))\n",
    "demo_files = conv_files[:1]\n",
    "print(f\"Tokenizer vocab : {len(tokenizer)}\")\n",
    "print(f\"Conversation shards found : {len(conv_files)}\")\n",
    "print(f\"Using for demo  : {[f.name for f in demo_files]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_yield_packed  : 4 chunks\n",
      "_yield_aligned : 4 chunks\n"
     ]
    }
   ],
   "source": [
    "N_CHUNKS   = 4\n",
    "SEQ_LENGTH = 2048\n",
    "SOURCE     = 'smoltalk'\n",
    "\n",
    "# mask_policy='all'            → _yield_packed  (linear, no boundary alignment)\n",
    "# mask_policy='assistant_only' → _yield_bos_aligned (each chunk starts at a conversation boundary)\n",
    "\n",
    "ds_packed = PackedStreamingDataset(\n",
    "    demo_files, tokenizer, seq_length=SEQ_LENGTH,\n",
    "    rank=0, world_size=1, shuffle=False, max_sequences=N_CHUNKS,\n",
    "    data_format='conversation', source_filter=SOURCE,\n",
    "    mask_policy='all',\n",
    ")\n",
    "ds_aligned = PackedStreamingDataset(\n",
    "    demo_files, tokenizer, seq_length=SEQ_LENGTH,\n",
    "    rank=0, world_size=1, shuffle=False, max_sequences=N_CHUNKS,\n",
    "    data_format='conversation', source_filter=SOURCE,\n",
    "    mask_policy='assistant_only',\n",
    ")\n",
    "\n",
    "packed_chunks  = list(iter(ds_packed))\n",
    "aligned_chunks = list(iter(ds_aligned))\n",
    "\n",
    "print(f\"_yield_packed  : {len(packed_chunks)} chunks\")\n",
    "print(f\"_yield_aligned : {len(aligned_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  _yield_packed  (mask_policy=\"all\", linear stream-packing)\n",
      "============================================================\n",
      "  ── Chunk 1 ──────────────────────────────────────────────\n",
      "  ✓ starts at BOS : True   trained: 2046/2048 tokens (99.9%)\n",
      "    ▶ <[MID-CONV]> 'A tailor cut 0.75 inch off a skirt and some inches off a pair of pants. The tailor cut 0.2...'\n",
      "\n",
      "  ── Chunk 2 ──────────────────────────────────────────────\n",
      "  ✗ starts at BOS : False   trained: 2043/2048 tokens (99.8%)\n",
      "    ▶ <[MID-CONV]> 'of the first corner. Defaults to 0.\"},\"x2\":{\"type\":\"integer\",\"description\":\"The x-coordina...'\n",
      "\n",
      "  ── Chunk 3 ──────────────────────────────────────────────\n",
      "  ✗ starts at BOS : False   trained: 2046/2048 tokens (99.9%)\n",
      "    ▶ <[MID-CONV]> 'is to change \"suggest\" to \"suggests\" because \"data\" is a singular noun. However, I\\'ve seen...'\n",
      "\n",
      "  ── Chunk 4 ──────────────────────────────────────────────\n",
      "  ✗ starts at BOS : False   trained: 2045/2048 tokens (99.9%)\n",
      "    ▶ <[MID-CONV]> 'integer `10`. The memory is deleted when both `ptr1` and `ptr2` go out of scope.\\n\\n3. **wea...'\n",
      "\n",
      "============================================================\n",
      "  _yield_bos_aligned  (mask_policy=\"assistant_only\")\n",
      "============================================================\n",
      "  ── Chunk 1 ──────────────────────────────────────────────\n",
      "  ✓ starts at BOS : True   trained: 1741/2048 tokens (85.0%)\n",
      "    ▶ <[MID-CONV]> 'A tailor cut 0.75 inch off a skirt and some inches off a pair of pants. The tailor cut 0.2...'\n",
      "\n",
      "  ── Chunk 2 ──────────────────────────────────────────────\n",
      "  ✓ starts at BOS : True   trained: 936/2048 tokens (45.7%)\n",
      "    ▶ <[MID-CONV]> 'You are an expert in composing functions. You are given a question and a set of possible f...'\n",
      "\n",
      "  ── Chunk 3 ──────────────────────────────────────────────\n",
      "  ✓ starts at BOS : True   trained: 508/2048 tokens (24.8%)\n",
      "    ▶ <[MID-CONV]> 'You are an expert in composing functions. You are given a question and a set of possible f...'\n",
      "\n",
      "  ── Chunk 4 ──────────────────────────────────────────────\n",
      "  ✓ starts at BOS : True   trained: 1709/2048 tokens (83.4%)\n",
      "    ▶ <[MID-CONV]> 'How do you debug and fix memory leaks?\\nDebugging memory leaks can be a complex task, but i...'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_chunk(chunk, idx=0, max_chars=90):\n",
    "    \"\"\"Show conversation structure of a chunk with train/masked annotations.\"\"\"\n",
    "    ids  = list(chunk['input_ids'])\n",
    "    mask = list(chunk['loss_mask'])\n",
    "\n",
    "    n_trained    = int(sum(mask))\n",
    "    first_is_bos = (ids[0] == bos_id)\n",
    "\n",
    "    # Walk tokens and group into role-delimited sections\n",
    "    sections = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        tok = ids[i]\n",
    "        if tok in BOUNDARY_IDS:\n",
    "            role = ROLE_NAMES[tok]\n",
    "            role_trained = (mask[i] == 1)\n",
    "            j = i + 1\n",
    "            while j < len(ids) and ids[j] not in BOUNDARY_IDS:\n",
    "                j += 1\n",
    "            content_ids  = ids[i+1:j]\n",
    "            content_mask = mask[i+1:j]\n",
    "            trained = role_trained or any(m == 1 for m in content_mask)\n",
    "            text = ''\n",
    "            if content_ids:\n",
    "                text = tokenizer.decode(content_ids, skip_special_tokens=True).strip()\n",
    "                text = text[:max_chars] + ('...' if len(text) > max_chars else '')\n",
    "            sections.append((role, text, trained))\n",
    "            i = j\n",
    "        else:\n",
    "            # Tokens with no preceding role boundary — only happens in _yield_packed\n",
    "            # when a chunk starts mid-conversation\n",
    "            j = i\n",
    "            while j < len(ids) and ids[j] not in BOUNDARY_IDS:\n",
    "                j += 1\n",
    "            text = tokenizer.decode(ids[i:j], skip_special_tokens=True).strip()\n",
    "            text = text[:max_chars] + ('...' if len(text) > max_chars else '')\n",
    "            trained = any(m == 1 for m in mask[i:j])\n",
    "            sections.append(('[MID-CONV]', text, trained))\n",
    "            i = j\n",
    "\n",
    "    print(f\"  ── Chunk {idx+1} {'─'*46}\")\n",
    "    ok = '✓' if first_is_bos else '✗'\n",
    "    print(f\"  {ok} starts at BOS : {first_is_bos}   \"\n",
    "          f\"trained: {n_trained}/{len(mask)} tokens ({100*n_trained/len(mask):.1f}%)\")\n",
    "    for role, text, trained in sections:\n",
    "        marker = '  ▶' if trained else '   '\n",
    "        print(f\"  {marker} <{role:<9}> {repr(text)}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print('=' * 60)\n",
    "print('  _yield_packed  (mask_policy=\"all\", linear stream-packing)')\n",
    "print('=' * 60)\n",
    "for i, c in enumerate(packed_chunks):\n",
    "    show_chunk(c, idx=i)\n",
    "\n",
    "print('=' * 60)\n",
    "print('  _yield_bos_aligned  (mask_policy=\"assistant_only\")')\n",
    "print('=' * 60)\n",
    "for i, c in enumerate(aligned_chunks):\n",
    "    show_chunk(c, idx=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT BEFORE FIRST TRAINED LABEL\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "  ✗ packed   Chunk 1:\n",
      "       role tokens seen : []\n",
      "       user context     : ''\n",
      "       first prediction : '<|user|>'\n",
      "       ⚠ no BOS — chunk starts mid-conversation, user question is not in context\n",
      "\n",
      "  ✗ packed   Chunk 2:\n",
      "       role tokens seen : []\n",
      "       user context     : ''\n",
      "       first prediction : ' the'\n",
      "       ⚠ no BOS — chunk starts mid-conversation, user question is not in context\n",
      "\n",
      "  ✗ packed   Chunk 3:\n",
      "       role tokens seen : []\n",
      "       user context     : ''\n",
      "       first prediction : ' to'\n",
      "       ⚠ no BOS — chunk starts mid-conversation, user question is not in context\n",
      "\n",
      "  ✗ packed   Chunk 4:\n",
      "       role tokens seen : []\n",
      "       user context     : ''\n",
      "       first prediction : ' `'\n",
      "       ⚠ no BOS — chunk starts mid-conversation, user question is not in context\n"
     ]
    }
   ],
   "source": [
    "# Summary: for each chunk, show what the model sees before its first trained label\n",
    "\n",
    "print('CONTEXT BEFORE FIRST TRAINED LABEL')\n",
    "print('─' * 60)\n",
    "for label, chunks in [('packed ', packed_chunks), ('aligned', aligned_chunks)]:\n",
    "    for i, c in enumerate(chunks):\n",
    "        ids  = list(c['input_ids'])\n",
    "        lbl  = list(c['labels'])\n",
    "        mask = list(c['loss_mask'])\n",
    "        first_pos = next((j for j, m in enumerate(mask) if m == 1), None)\n",
    "        if first_pos is None:\n",
    "            print(f\"  {label}  Chunk {i+1}: no trained tokens\")\n",
    "            continue\n",
    "        context_ids    = ids[:first_pos]\n",
    "        first_label_id = lbl[first_pos]\n",
    "        has_bos        = (bos_id in context_ids)\n",
    "        has_usr        = (usr_id in context_ids)\n",
    "        ok = '✓' if (has_bos and has_usr) else '✗'\n",
    "        context_text = tokenizer.decode(\n",
    "            [t for t in context_ids if t not in BOUNDARY_IDS],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()[:80]\n",
    "        first_label_text = tokenizer.decode([first_label_id], skip_special_tokens=False)\n",
    "        role_sequence = [ROLE_NAMES[t] for t in context_ids if t in BOUNDARY_IDS]\n",
    "        print(f\"\\n  {ok} {label}  Chunk {i+1}:\")\n",
    "        print(f\"       role tokens seen : {role_sequence}\")\n",
    "        print(f\"       user context     : {repr(context_text)}\")\n",
    "        print(f\"       first prediction : {repr(first_label_text)}\")\n",
    "        if not has_bos:\n",
    "            print(f\"       ⚠ no BOS — chunk starts mid-conversation, user question is not in context\")\n",
    "        elif not has_usr:\n",
    "            print(f\"       ⚠ no USR  — assistant response has no preceding user turn in context\")\n",
    "        else:\n",
    "            print(f\"       ✓ full user turn visible before first trained token\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
