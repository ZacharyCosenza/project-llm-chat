{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da12610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Tiny GPT model\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size=100, dim=64, n_layers=2, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_emb = nn.Embedding(128, dim)  # max seq len 128\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=dim, \n",
    "                nhead=n_heads, \n",
    "                dim_feedforward=dim*4,\n",
    "                batch_first=True,\n",
    "                norm_first=True  # Pre-LN like GPT\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        tok_emb = self.tok_emb(x)  # [B, T, dim]\n",
    "        pos_emb = self.pos_emb(torch.arange(T, device=x.device))  # [T, dim]\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Transformer blocks (with causal mask)\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(T, device=x.device)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, src_mask=causal_mask, is_causal=True)\n",
    "        \n",
    "        # Output\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # [B, T, vocab_size]\n",
    "        return logits\n",
    "\n",
    "# Smoke test\n",
    "def smoke_test():\n",
    "    print(\"ðŸ”¥ GPT Smoke Test\")\n",
    "    \n",
    "    # Setup\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    vocab_size = 100\n",
    "    model = TinyGPT(vocab_size=vocab_size).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Fake data: learn to copy input shifted by 1\n",
    "    # Input:  [1, 2, 3, 4, 5]\n",
    "    # Target: [2, 3, 4, 5, 6]\n",
    "    def make_batch(batch_size=4, seq_len=16):\n",
    "        x = torch.randint(0, vocab_size-1, (batch_size, seq_len), device=device)\n",
    "        y = x + 1  # target is input shifted by 1 in vocab\n",
    "        return x, y\n",
    "    \n",
    "    # Train for 100 steps\n",
    "    print(\"\\nðŸ“š Training...\")\n",
    "    losses = []\n",
    "    for step in range(100):\n",
    "        x, y = make_batch()\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Step {step:3d} | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Test: can it learn the pattern?\n",
    "    print(\"\\nðŸ§ª Testing...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_x = torch.tensor([[1, 2, 3, 4, 5]], device=device)\n",
    "        logits = model(test_x)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        \n",
    "        print(f\"Input:     {test_x[0].tolist()}\")\n",
    "        print(f\"Predicted: {preds[0].tolist()}\")\n",
    "        print(f\"Expected:  {(test_x[0] + 1).tolist()}\")\n",
    "        \n",
    "        accuracy = (preds[0] == test_x[0] + 1).float().mean()\n",
    "        print(f\"Accuracy:  {accuracy:.2%}\")\n",
    "    \n",
    "    # Check overfitting\n",
    "    final_loss = sum(losses[-10:]) / 10\n",
    "    print(f\"\\nâœ… Final loss: {final_loss:.4f}\")\n",
    "    \n",
    "    if final_loss < 0.1:\n",
    "        print(\"âœ… PASS: Model learned the pattern!\")\n",
    "    else:\n",
    "        print(\"âŒ FAIL: Model didn't converge\")\n",
    "    \n",
    "    return final_loss < 0.1\n",
    "\n",
    "success = smoke_test()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
