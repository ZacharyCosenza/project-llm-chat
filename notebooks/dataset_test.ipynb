{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Average Tokens per Shard by Dataset\n",
    "\n",
    "Estimates the average number of tokens per parquet shard for each of the four dataset types:\n",
    "FineWeb-Edu, SmolTalk, UltraChat-Gen, and UltraChat-SFT.\n",
    "\n",
    "Samples N shards per dataset, tokenizes a random subset of rows from each shard, scales by\n",
    "the full shard row count to estimate total tokens, then averages across shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaccosenza/code/project-llm-chat/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 50262\n",
      "BOS=50257, EOS=50256, PAD=50258\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '<|beginoftext|>',\n",
    "    'pad_token': '<|pad|>',\n",
    "    'additional_special_tokens': ['<|user|>', '<|assistant|>', '<|system|>'],\n",
    "})\n",
    "print(f'Vocab: {len(tokenizer)}')\n",
    "print(f'BOS={tokenizer.bos_token_id}, EOS={tokenizer.eos_token_id}, PAD={tokenizer.pad_token_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu shards : 1706 total, sampling 10\n",
      "Conversation shards: 31 total, sampling 10\n",
      "LIMA files         : 1 total\n"
     ]
    }
   ],
   "source": [
    "N_SHARDS = 10   # shards to sample per dataset\n",
    "\n",
    "BASE_DIR      = os.path.join(os.getcwd(), '..', 'data')\n",
    "fineweb_dir   = os.path.join(BASE_DIR, 'base_data')\n",
    "conv_dir      = os.path.join(BASE_DIR, 'conversation_data')\n",
    "lima_dir      = os.path.join(BASE_DIR, 'lima_data')\n",
    "\n",
    "fineweb_files = sorted(f for f in os.listdir(fineweb_dir) if f.endswith('.parquet'))\n",
    "conv_files    = sorted(f for f in os.listdir(conv_dir)    if f.endswith('.parquet'))\n",
    "lima_files    = sorted(f for f in os.listdir(lima_dir) if f.endswith('.parquet')) if os.path.isdir(lima_dir) else []\n",
    "\n",
    "print(f'FineWeb-Edu shards : {len(fineweb_files)} total, sampling {N_SHARDS}')\n",
    "print(f'Conversation shards: {len(conv_files)} total, sampling {N_SHARDS}')\n",
    "print(f'LIMA files         : {len(lima_files)} total')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PER_SHARD = 4000  # rows to tokenize per shard (trades speed vs accuracy)\n",
    "\n",
    "role_tok = {\n",
    "    'user':      tokenizer.convert_tokens_to_ids('<|user|>'),\n",
    "    'assistant': tokenizer.convert_tokens_to_ids('<|assistant|>'),\n",
    "    'system':    tokenizer.convert_tokens_to_ids('<|system|>'),\n",
    "}\n",
    "\n",
    "def tokens_for_text(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def tokens_for_conv(messages):\n",
    "    \"\"\"BOS + (role_token + content tokens per turn) + EOS.\"\"\"\n",
    "    n = 2  # BOS + EOS\n",
    "    for msg in messages:\n",
    "        if msg['role'] in role_tok:\n",
    "            n += 1\n",
    "        n += len(tokenizer.encode(msg['content'], add_special_tokens=False))\n",
    "    return n\n",
    "\n",
    "def estimate_shard_tokens(rows, tok_fn, sample_size=SAMPLE_PER_SHARD):\n",
    "    \"\"\"Tokenize a random sample of rows, scale mean to full shard row count.\"\"\"\n",
    "    sample = random.sample(rows, min(sample_size, len(rows)))\n",
    "    mean_tok = np.mean([tok_fn(r) for r in sample])\n",
    "    return mean_tok * len(rows), mean_tok, len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## FineWeb-Edu: tokens per shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1764 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  shard_01309.parquet: 54,272 docs, ~1014 tok/doc → 55.0M tokens\n",
      "  shard_00228.parquet: 54,272 docs, ~1019 tok/doc → 55.3M tokens\n",
      "  shard_00051.parquet: 53,248 docs, ~1000 tok/doc → 53.3M tokens\n",
      "  shard_01518.parquet: 53,248 docs, ~1040 tok/doc → 55.4M tokens\n",
      "  shard_00563.parquet: 53,248 docs, ~1055 tok/doc → 56.2M tokens\n",
      "  shard_00501.parquet: 53,248 docs, ~1053 tok/doc → 56.1M tokens\n",
      "  shard_00457.parquet: 53,248 docs, ~1019 tok/doc → 54.2M tokens\n",
      "  shard_00285.parquet: 53,248 docs, ~991 tok/doc → 52.8M tokens\n",
      "  shard_01508.parquet: 53,248 docs, ~1010 tok/doc → 53.8M tokens\n",
      "  shard_00209.parquet: 53,248 docs, ~1064 tok/doc → 56.7M tokens\n",
      "\n",
      "FineWeb-Edu  avg tokens/shard: 54.9M ± 1.2M  (N=10)\n"
     ]
    }
   ],
   "source": [
    "fw_sample = random.sample(fineweb_files, N_SHARDS)\n",
    "fw_shard_tokens = []\n",
    "\n",
    "for fname in fw_sample:\n",
    "    path = os.path.join(fineweb_dir, fname)\n",
    "    texts = pq.read_table(path)['text'].to_pylist()\n",
    "    total, mean, n = estimate_shard_tokens(texts, tokens_for_text)\n",
    "    fw_shard_tokens.append(total)\n",
    "    print(f'  {fname}: {n:,} docs, ~{mean:.0f} tok/doc → {total/1e6:.1f}M tokens')\n",
    "\n",
    "fw_avg = np.mean(fw_shard_tokens)\n",
    "fw_std = np.std(fw_shard_tokens)\n",
    "print(f'\\nFineWeb-Edu  avg tokens/shard: {fw_avg/1e6:.1f}M ± {fw_std/1e6:.1f}M  (N={N_SHARDS})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Conversation datasets: tokens per shard\n",
    "\n",
    "Each conversation shard contains rows from multiple sources. For each sampled shard,\n",
    "rows are split by source, sampled independently, and scaled to estimate per-source token totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard_00023.parquet:\n",
      "  smoltalk              : 34,513 convs, ~866 tok/conv → 29.9M tokens\n",
      "  ultrachat_gen         : 8,523 convs, ~901 tok/conv → 7.7M tokens\n",
      "  ultrachat_sft         : 6,964 convs, ~1260 tok/conv → 8.8M tokens\n",
      "\n",
      "shard_00008.parquet:\n",
      "  smoltalk              : 34,734 convs, ~893 tok/conv → 31.0M tokens\n",
      "  ultrachat_gen         : 8,457 convs, ~955 tok/conv → 8.1M tokens\n",
      "  ultrachat_sft         : 6,809 convs, ~1223 tok/conv → 8.3M tokens\n",
      "\n",
      "shard_00027.parquet:\n",
      "  smoltalk              : 34,494 convs, ~962 tok/conv → 33.2M tokens\n",
      "  ultrachat_gen         : 8,610 convs, ~934 tok/conv → 8.0M tokens\n",
      "  ultrachat_sft         : 6,896 convs, ~1219 tok/conv → 8.4M tokens\n",
      "\n",
      "shard_00016.parquet:\n",
      "  smoltalk              : 34,619 convs, ~929 tok/conv → 32.1M tokens\n",
      "  ultrachat_gen         : 8,424 convs, ~893 tok/conv → 7.5M tokens\n",
      "  ultrachat_sft         : 6,957 convs, ~1212 tok/conv → 8.4M tokens\n",
      "\n",
      "shard_00018.parquet:\n",
      "  smoltalk              : 34,459 convs, ~886 tok/conv → 30.5M tokens\n",
      "  ultrachat_gen         : 8,572 convs, ~902 tok/conv → 7.7M tokens\n",
      "  ultrachat_sft         : 6,969 convs, ~1202 tok/conv → 8.4M tokens\n",
      "\n",
      "shard_00025.parquet:\n",
      "  smoltalk              : 34,536 convs, ~914 tok/conv → 31.6M tokens\n",
      "  ultrachat_gen         : 8,513 convs, ~903 tok/conv → 7.7M tokens\n",
      "  ultrachat_sft         : 6,951 convs, ~1225 tok/conv → 8.5M tokens\n",
      "\n",
      "shard_00024.parquet:\n",
      "  smoltalk              : 34,734 convs, ~909 tok/conv → 31.6M tokens\n",
      "  ultrachat_gen         : 8,450 convs, ~908 tok/conv → 7.7M tokens\n",
      "  ultrachat_sft         : 6,816 convs, ~1231 tok/conv → 8.4M tokens\n",
      "\n",
      "shard_00010.parquet:\n",
      "  smoltalk              : 34,492 convs, ~944 tok/conv → 32.6M tokens\n",
      "  ultrachat_gen         : 8,575 convs, ~936 tok/conv → 8.0M tokens\n",
      "  ultrachat_sft         : 6,933 convs, ~1225 tok/conv → 8.5M tokens\n",
      "\n",
      "shard_00007.parquet:\n",
      "  smoltalk              : 34,409 convs, ~969 tok/conv → 33.3M tokens\n",
      "  ultrachat_gen         : 8,537 convs, ~861 tok/conv → 7.4M tokens\n",
      "  ultrachat_sft         : 7,054 convs, ~1251 tok/conv → 8.8M tokens\n",
      "\n",
      "shard_00017.parquet:\n",
      "  smoltalk              : 34,486 convs, ~903 tok/conv → 31.1M tokens\n",
      "  ultrachat_gen         : 8,539 convs, ~916 tok/conv → 7.8M tokens\n",
      "  ultrachat_sft         : 6,975 convs, ~1225 tok/conv → 8.5M tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SOURCES = ['smoltalk', 'ultrachat_gen', 'ultrachat_sft']\n",
    "\n",
    "cv_sample = random.sample(conv_files, N_SHARDS)\n",
    "shard_totals = {src: [] for src in SOURCES}\n",
    "\n",
    "for fname in cv_sample:\n",
    "    path = os.path.join(conv_dir, fname)\n",
    "    rows = pq.read_table(path).to_pylist()\n",
    "\n",
    "    by_source = {src: [] for src in SOURCES}\n",
    "    for row in rows:\n",
    "        src = row['source']\n",
    "        if src in by_source:\n",
    "            by_source[src].append(row['messages'])\n",
    "\n",
    "    print(f'{fname}:')\n",
    "    for src in SOURCES:\n",
    "        msgs = by_source[src]\n",
    "        if not msgs:\n",
    "            shard_totals[src].append(0.0)\n",
    "            print(f'  {src:<22}: 0 rows')\n",
    "            continue\n",
    "        total, mean, n = estimate_shard_tokens(msgs, tokens_for_conv, sample_size=1000)\n",
    "        shard_totals[src].append(total)\n",
    "        print(f'  {src:<22}: {n:,} convs, ~{mean:.0f} tok/conv → {total/1e6:.1f}M tokens')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rzgdfbjcrpe",
   "metadata": {},
   "source": [
    "## LIMA: tokens (single file, ~1K conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "gkqgab2hdh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIMA: 1030 convs, ~663 tok/conv → 683.4K tokens total\n",
      "  min=29, max=3654, median=459\n"
     ]
    }
   ],
   "source": [
    "if lima_files:\n",
    "    lima_path = os.path.join(lima_dir, lima_files[0])\n",
    "    lima_rows = pq.read_table(lima_path).to_pylist()\n",
    "    # Tokenize all rows (small enough to do fully)\n",
    "    lima_tok_counts = [tokens_for_conv(r['messages']) for r in lima_rows]\n",
    "    lima_mean = np.mean(lima_tok_counts)\n",
    "    lima_total = sum(lima_tok_counts)\n",
    "    print(f'LIMA: {len(lima_rows)} convs, ~{lima_mean:.0f} tok/conv → {lima_total/1e3:.1f}K tokens total')\n",
    "    print(f'  min={min(lima_tok_counts)}, max={max(lima_tok_counts)}, median={int(np.median(lima_tok_counts))}')\n",
    "else:\n",
    "    lima_rows, lima_mean, lima_total = [], 0, 0\n",
    "    print('LIMA not found — run: python -m core.dataset --lima')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset                   Avg tokens/shard       Std  N shards\n",
      "--------------------------------------------------------------\n",
      "fineweb_edu                        54.9M    1.2M        10\n",
      "smoltalk                           31.7M    1.1M        10\n",
      "ultrachat_gen                       7.8M    0.2M        10\n",
      "ultrachat_sft                       8.5M    0.2M        10\n",
      "lima                             683.4K       —     1 file  (1030 convs total)\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"Dataset\":<22}  {\"Avg tokens/shard\":>18}  {\"Std\":>8}  {\"N shards\":>8}')\n",
    "print('-' * 62)\n",
    "print(f'{\"fineweb_edu\":<22}  {fw_avg/1e6:>15.1f}M  {fw_std/1e6:>5.1f}M  {N_SHARDS:>8}')\n",
    "for src in SOURCES:\n",
    "    vals = np.array(shard_totals[src])\n",
    "    avg  = vals.mean()\n",
    "    std  = vals.std()\n",
    "    n    = int((vals > 0).sum())\n",
    "    print(f'{src:<22}  {avg/1e6:>15.1f}M  {std/1e6:>5.1f}M  {n:>8}')\n",
    "if lima_rows:\n",
    "    print(f'{\"lima\":<22}  {lima_total/1e3:>14.1f}K  {\"—\":>6}   {\"1 file\":>8}  ({len(lima_rows)} convs total)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ob6i9e7q1s",
   "metadata": {},
   "source": [
    "## One example per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ak8uygbkytq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "  fineweb_edu  [shard_00259.parquet]\n",
      "================================================================\n",
      "  In dry economic terms, consumer credit is a certain amount of money that is transferred to temporary use by households to purchase consumer goods. In other words, this is an Overdraft, which is issued to citizens (not legal entities) for the purchase of certain goods.\n",
      "Consumer credit can be targeted (it is issued for the purchase of a specific product) and not targeted – in this case, money is spe...\n",
      "\n",
      "================================================================\n",
      "  smoltalk\n",
      "================================================================\n",
      "  <SYSTEM>\n",
      "  You're an AI assistant for text re-writing. Rewrite the input text to make it more friendly and approachable while maintaining its main points.\n",
      "\n",
      "  <USER>\n",
      "  Dr. Thompson,\n",
      "\n",
      "I reviewed the latest draft of our joint publication and found several grammatical inconsistencies that need to be addressed. For instance, the use of \"lay\" and \"lie\" is frequently misused, and the sentence structure in the third paragraph is convoluted and difficult to follow.\n",
      "\n",
      "I suggest we meet to go over these issues in detail. Your approach to grammar seems a bit lax, and it’s i...\n",
      "\n",
      "  <ASSISTANT>\n",
      "  Hi Dr. Thompson,\n",
      "\n",
      "I hope you're doing well! I've had a chance to review the latest draft of our joint publication, and I noticed a few areas where we can improve the clarity and consistency of our writing. For example, there are a few instances where \"lay\" and \"lie\" are mixed up, and the third paragraph could use a bit of restructuring to make it easier to read.\n",
      "\n",
      "I think it would be great if we co...\n",
      "\n",
      "\n",
      "================================================================\n",
      "  ultrachat_gen\n",
      "================================================================\n",
      "  <USER>\n",
      "  Attraction marketing success is doing certain things so that people come to you versus you seeking them out. But it doesn’t need to be complicated because it’s a very simple concept. Ninety-five of network marketers don’t use this form of marketing. Ninety-five of people are not the one’s you see on the stage at your company’s awards ceremony receiving their 6 or 7 figure bonus check.\n",
      "MLM is a peo...\n",
      "\n",
      "  <ASSISTANT>\n",
      "  Posting articles with targeted keywords and keyword density can help with attraction marketing by driving traffic to your blog. This attracts highly qualified prospects who are interested in what you have to offer, making it more likely that they will join your network marketing business. Additionally, by providing helpful and valuable information in your articles, prospects will begin to trust an...\n",
      "\n",
      "  <USER>\n",
      "  Can you provide more information about how MLSP can help me with my attraction marketing efforts?\n",
      "\n",
      "\n",
      "================================================================\n",
      "  ultrachat_sft\n",
      "================================================================\n",
      "  <USER>\n",
      "  Write a 1,000-word third-person narrative story about a character (male or female) who discovers their magical abilities and sets out on a journey to become a powerful witch or wizard. Incorporate themes of self-discovery, mentorship, and the challenges of mastering magic. Use vivid descriptions to transport the reader to fantastical lands and settings filled with mystical creatures and magical sp...\n",
      "\n",
      "  <ASSISTANT>\n",
      "  In a small village hidden deep within the forest, lived a young girl named Luna. She had always known that she was different, but she never knew why. Luna always had a fascination with the stars and the moon. She would spend endless hours gazing up at the night sky, wondering what it would be like to touch the stars and feel their magic.\n",
      "\n",
      "One night, while she was lost in thought, Luna noticed some...\n",
      "\n",
      "  <USER>\n",
      "  Wow, that was an amazing story! The journey that Luna went through to become a powerful witch and to defeat the dark mage was captivating. I would love to read more about the mystical world she found herself in. Can you expand further on the different creatures Luna and Selena encountered along the way?\n",
      "\n",
      "  <ASSISTANT>\n",
      "  Certainly! As Luna and Selena journeyed through the forest, they encountered a variety of mythical creatures that tested their skills in magic and combat.\n",
      "\n",
      "One of the first creatures they encountered was a group of fairies. These wee sprites were mischievous, flitting about the trees and playing pranks on unwary travelers. But as Luna and Selena soon discovered, fairies held incredible power. They...\n",
      "\n",
      "\n",
      "================================================================\n",
      "  lima  (source=lima)\n",
      "================================================================\n",
      "  <USER>\n",
      "  Give me a response email for a customer's request to get refunded for a computer monitor that arrived with a crack.\n",
      "\n",
      "  <ASSISTANT>\n",
      "  Hi [Recipient's Name],\n",
      "\n",
      "Thanks for getting in touch with us! We apologize for the issue you had with your recent order. We totally understand your decision and would be happy to assist you with returning your monitor for a refund.\n",
      "\n",
      "Please let us know if there was anything we could have done better to improve your shopping experience with us. Your feedback is valuable and will help us serve our cus...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_TURN_CHARS = 400  # truncate long turns for readability\n",
    "\n",
    "def print_conv(messages, label):\n",
    "    print(f'\\n{\"=\" * 64}')\n",
    "    print(f'  {label}')\n",
    "    print(f'{\"=\" * 64}')\n",
    "    for msg in messages:\n",
    "        role = msg['role'].upper()\n",
    "        text = msg['content'].strip()\n",
    "        if len(text) > MAX_TURN_CHARS:\n",
    "            text = text[:MAX_TURN_CHARS] + '...'\n",
    "        print(f'  <{role}>\\n  {text}\\n')\n",
    "\n",
    "# ── FineWeb-Edu ──────────────────────────────────────────────────────────────\n",
    "fw_example_file = random.choice(fineweb_files)\n",
    "fw_texts = pq.read_table(os.path.join(fineweb_dir, fw_example_file))['text'].to_pylist()\n",
    "fw_text = random.choice(fw_texts).strip()\n",
    "print(f'\\n{\"=\" * 64}')\n",
    "print(f'  fineweb_edu  [{fw_example_file}]')\n",
    "print(f'{\"=\" * 64}')\n",
    "print(f'  {fw_text[:MAX_TURN_CHARS]}{\"...\" if len(fw_text) > MAX_TURN_CHARS else \"\"}')\n",
    "\n",
    "# ── Conversation sources (SmolTalk / UltraChat-Gen / UltraChat-SFT) ──────────\n",
    "by_source = {src: [] for src in SOURCES}\n",
    "for fname in random.sample(conv_files, min(3, len(conv_files))):\n",
    "    for row in pq.read_table(os.path.join(conv_dir, fname)).to_pylist():\n",
    "        if row['source'] in by_source and len(by_source[row['source']]) < 200:\n",
    "            by_source[row['source']].append(row['messages'])\n",
    "\n",
    "for src in SOURCES:\n",
    "    if by_source[src]:\n",
    "        print_conv(random.choice(by_source[src]), src)\n",
    "    else:\n",
    "        print(f'\\n[{src}] — no rows found in sampled shards')\n",
    "\n",
    "# ── LIMA ─────────────────────────────────────────────────────────────────────\n",
    "if lima_rows:\n",
    "    example = random.choice(lima_rows)\n",
    "    print_conv(example['messages'], f'lima  (source={example[\"source\"]})')\n",
    "else:\n",
    "    print('\\n[lima] — not downloaded yet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
