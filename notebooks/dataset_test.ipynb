{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Average Tokens per Shard by Dataset\n",
    "\n",
    "Estimates the average number of tokens per parquet shard for each of the four dataset types:\n",
    "FineWeb-Edu, SmolTalk, UltraChat-Gen, and UltraChat-SFT.\n",
    "\n",
    "Samples N shards per dataset, tokenizes a random subset of rows from each shard, scales by\n",
    "the full shard row count to estimate total tokens, then averages across shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaccosenza/code/project-llm-chat/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 50262\n",
      "BOS=50257, EOS=50256, PAD=50258\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '<|beginoftext|>',\n",
    "    'pad_token': '<|pad|>',\n",
    "    'additional_special_tokens': ['<|user|>', '<|assistant|>', '<|system|>'],\n",
    "})\n",
    "print(f'Vocab: {len(tokenizer)}')\n",
    "print(f'BOS={tokenizer.bos_token_id}, EOS={tokenizer.eos_token_id}, PAD={tokenizer.pad_token_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineWeb-Edu shards : 1706 total, sampling 10\n",
      "Conversation shards: 31 total, sampling 10\n"
     ]
    }
   ],
   "source": [
    "N_SHARDS = 10   # shards to sample per dataset\n",
    "\n",
    "BASE_DIR      = os.path.join(os.getcwd(), '..', 'data')\n",
    "fineweb_dir   = os.path.join(BASE_DIR, 'base_data')\n",
    "conv_dir      = os.path.join(BASE_DIR, 'conversation_data')\n",
    "\n",
    "fineweb_files = sorted(f for f in os.listdir(fineweb_dir) if f.endswith('.parquet'))\n",
    "conv_files    = sorted(f for f in os.listdir(conv_dir)    if f.endswith('.parquet'))\n",
    "\n",
    "print(f'FineWeb-Edu shards : {len(fineweb_files)} total, sampling {N_SHARDS}')\n",
    "print(f'Conversation shards: {len(conv_files)} total, sampling {N_SHARDS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PER_SHARD = 4000  # rows to tokenize per shard (trades speed vs accuracy)\n",
    "\n",
    "role_tok = {\n",
    "    'user':      tokenizer.convert_tokens_to_ids('<|user|>'),\n",
    "    'assistant': tokenizer.convert_tokens_to_ids('<|assistant|>'),\n",
    "    'system':    tokenizer.convert_tokens_to_ids('<|system|>'),\n",
    "}\n",
    "\n",
    "def tokens_for_text(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def tokens_for_conv(messages):\n",
    "    \"\"\"BOS + (role_token + content tokens per turn) + EOS.\"\"\"\n",
    "    n = 2  # BOS + EOS\n",
    "    for msg in messages:\n",
    "        if msg['role'] in role_tok:\n",
    "            n += 1\n",
    "        n += len(tokenizer.encode(msg['content'], add_special_tokens=False))\n",
    "    return n\n",
    "\n",
    "def estimate_shard_tokens(rows, tok_fn, sample_size=SAMPLE_PER_SHARD):\n",
    "    \"\"\"Tokenize a random sample of rows, scale mean to full shard row count.\"\"\"\n",
    "    sample = random.sample(rows, min(sample_size, len(rows)))\n",
    "    mean_tok = np.mean([tok_fn(r) for r in sample])\n",
    "    return mean_tok * len(rows), mean_tok, len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## FineWeb-Edu: tokens per shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1764 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  shard_01309.parquet: 54,272 docs, ~1014 tok/doc → 55.0M tokens\n",
      "  shard_00228.parquet: 54,272 docs, ~1019 tok/doc → 55.3M tokens\n",
      "  shard_00051.parquet: 53,248 docs, ~1000 tok/doc → 53.3M tokens\n",
      "  shard_01518.parquet: 53,248 docs, ~1040 tok/doc → 55.4M tokens\n",
      "  shard_00563.parquet: 53,248 docs, ~1055 tok/doc → 56.2M tokens\n",
      "  shard_00501.parquet: 53,248 docs, ~1053 tok/doc → 56.1M tokens\n",
      "  shard_00457.parquet: 53,248 docs, ~1019 tok/doc → 54.2M tokens\n",
      "  shard_00285.parquet: 53,248 docs, ~991 tok/doc → 52.8M tokens\n",
      "  shard_01508.parquet: 53,248 docs, ~1010 tok/doc → 53.8M tokens\n",
      "  shard_00209.parquet: 53,248 docs, ~1064 tok/doc → 56.7M tokens\n",
      "\n",
      "FineWeb-Edu  avg tokens/shard: 54.9M ± 1.2M  (N=10)\n"
     ]
    }
   ],
   "source": [
    "fw_sample = random.sample(fineweb_files, N_SHARDS)\n",
    "fw_shard_tokens = []\n",
    "\n",
    "for fname in fw_sample:\n",
    "    path = os.path.join(fineweb_dir, fname)\n",
    "    texts = pq.read_table(path)['text'].to_pylist()\n",
    "    total, mean, n = estimate_shard_tokens(texts, tokens_for_text)\n",
    "    fw_shard_tokens.append(total)\n",
    "    print(f'  {fname}: {n:,} docs, ~{mean:.0f} tok/doc → {total/1e6:.1f}M tokens')\n",
    "\n",
    "fw_avg = np.mean(fw_shard_tokens)\n",
    "fw_std = np.std(fw_shard_tokens)\n",
    "print(f'\\nFineWeb-Edu  avg tokens/shard: {fw_avg/1e6:.1f}M ± {fw_std/1e6:.1f}M  (N={N_SHARDS})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Conversation datasets: tokens per shard\n",
    "\n",
    "Each conversation shard contains rows from multiple sources. For each sampled shard,\n",
    "rows are split by source, sampled independently, and scaled to estimate per-source token totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard_00023.parquet:\n",
      "  smoltalk              : 34,513 convs, ~866 tok/conv → 29.9M tokens\n",
      "  ultrachat_gen         : 8,523 convs, ~901 tok/conv → 7.7M tokens\n",
      "  ultrachat_sft         : 6,964 convs, ~1260 tok/conv → 8.8M tokens\n",
      "\n",
      "shard_00008.parquet:\n",
      "  smoltalk              : 34,734 convs, ~893 tok/conv → 31.0M tokens\n",
      "  ultrachat_gen         : 8,457 convs, ~955 tok/conv → 8.1M tokens\n",
      "  ultrachat_sft         : 6,809 convs, ~1223 tok/conv → 8.3M tokens\n",
      "\n",
      "shard_00027.parquet:\n",
      "  smoltalk              : 34,494 convs, ~962 tok/conv → 33.2M tokens\n",
      "  ultrachat_gen         : 8,610 convs, ~934 tok/conv → 8.0M tokens\n",
      "  ultrachat_sft         : 6,896 convs, ~1219 tok/conv → 8.4M tokens\n",
      "\n",
      "shard_00016.parquet:\n",
      "  smoltalk              : 34,619 convs, ~929 tok/conv → 32.1M tokens\n",
      "  ultrachat_gen         : 8,424 convs, ~893 tok/conv → 7.5M tokens\n",
      "  ultrachat_sft         : 6,957 convs, ~1212 tok/conv → 8.4M tokens\n",
      "\n",
      "shard_00018.parquet:\n",
      "  smoltalk              : 34,459 convs, ~886 tok/conv → 30.5M tokens\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<22\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: 0 rows\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m total, mean, n = \u001b[43mestimate_shard_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_for_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m shard_totals[src].append(total)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<22\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m convs, ~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tok/conv → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal/\u001b[32m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mM tokens\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mestimate_shard_tokens\u001b[39m\u001b[34m(rows, tok_fn, sample_size)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Tokenize a random sample of rows, scale mean to full shard row count.\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m sample = random.sample(rows, \u001b[38;5;28mmin\u001b[39m(sample_size, \u001b[38;5;28mlen\u001b[39m(rows)))\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m mean_tok = np.mean([\u001b[43mtok_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sample])\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mean_tok * \u001b[38;5;28mlen\u001b[39m(rows), mean_tok, \u001b[38;5;28mlen\u001b[39m(rows)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtokens_for_conv\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg[\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m role_tok:\n\u001b[32m     17\u001b[39m         n += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     n += \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/project-llm-chat/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2829\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[39m\n\u001b[32m   2810\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2811\u001b[39m \u001b[33;03m    Converts a string into a sequence of tokens, replacing unknown tokens with the `unk_token`.\u001b[39;00m\n\u001b[32m   2812\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2825\u001b[39m \u001b[33;03m        `list[str]`: The list of tokens.\u001b[39;00m\n\u001b[32m   2826\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2827\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2829\u001b[39m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[32m   2830\u001b[39m     ENCODE_KWARGS_DOCSTRING,\n\u001b[32m   2831\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2832\u001b[39m \u001b[33;03m        **kwargs: Passed along to the `.tokenize()` method.\u001b[39;00m\n\u001b[32m   2833\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m,\n\u001b[32m   2834\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2835\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   2836\u001b[39m \u001b[33;03m        `list[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`: The tokenized ids of the text.\u001b[39;00m\n\u001b[32m   2837\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m,\n\u001b[32m   2838\u001b[39m )\n\u001b[32m   2839\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\n\u001b[32m   2840\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2841\u001b[39m     text: Union[TextInput, PreTokenizedInput, EncodedInput],\n\u001b[32m   2842\u001b[39m     text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2843\u001b[39m     add_special_tokens: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2844\u001b[39m     padding: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, PaddingStrategy] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2845\u001b[39m     truncation: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, TruncationStrategy, \u001b[38;5;28;01mNone\u001b[39;00m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2846\u001b[39m     max_length: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2847\u001b[39m     stride: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   2848\u001b[39m     padding_side: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2849\u001b[39m     return_tensors: Optional[Union[\u001b[38;5;28mstr\u001b[39m, TensorType]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2850\u001b[39m     **kwargs,\n\u001b[32m   2851\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m   2852\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2853\u001b[39m \u001b[33;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[32m   2854\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2865\u001b[39m \u001b[33;03m            method).\u001b[39;00m\n\u001b[32m   2866\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2867\u001b[39m     encoded_inputs = \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   2868\u001b[39m         text,\n\u001b[32m   2869\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2877\u001b[39m         **kwargs,\n\u001b[32m   2878\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "SOURCES = ['smoltalk', 'ultrachat_gen', 'ultrachat_sft']\n",
    "\n",
    "cv_sample = random.sample(conv_files, N_SHARDS)\n",
    "shard_totals = {src: [] for src in SOURCES}\n",
    "\n",
    "for fname in cv_sample:\n",
    "    path = os.path.join(conv_dir, fname)\n",
    "    rows = pq.read_table(path).to_pylist()\n",
    "\n",
    "    by_source = {src: [] for src in SOURCES}\n",
    "    for row in rows:\n",
    "        src = row['source']\n",
    "        if src in by_source:\n",
    "            by_source[src].append(row['messages'])\n",
    "\n",
    "    print(f'{fname}:')\n",
    "    for src in SOURCES:\n",
    "        msgs = by_source[src]\n",
    "        if not msgs:\n",
    "            shard_totals[src].append(0.0)\n",
    "            print(f'  {src:<22}: 0 rows')\n",
    "            continue\n",
    "        total, mean, n = estimate_shard_tokens(msgs, tokens_for_conv, sample_size=1000)\n",
    "        shard_totals[src].append(total)\n",
    "        print(f'  {src:<22}: {n:,} convs, ~{mean:.0f} tok/conv → {total/1e6:.1f}M tokens')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6994fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'In what ways do contemporary street art and public installation art challenge traditional notions of art and aesthetics, and how do they engage with and reflect the cultural and social climates of the communities where they are created? Additionally, how do factors such as accessibility, public space, and the use of non-traditional materials play a role in the creation and reception of these art forms?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Contemporary street art and public installation art challenge traditional notions of art and aesthetics by rejecting the idea that art is only meant for the elite and should be displayed in galleries and museums. Instead, these art forms are accessible to anyone who passes by them on the street or in public spaces. Additionally, they often incorporate non-traditional materials, such as spray paint, stencils, and found objects.\\n\\nThese art forms engage with and reflect the cultural and social climates of the communities where they are created by addressing issues that are relevant to those communities. For example, some street artists use their work to raise awareness about political issues or to celebrate local culture and identity. Public installation artists may create works that are interactive, inviting community members to participate in the creation or use of the art.\\n\\nAccessibility plays a major role in the creation and reception of street art and public installation art. The fact that these art forms are displayed in public spaces means that they are often seen by a wider audience than traditional art forms. Additionally, the use of non-traditional materials and techniques may make these art forms more accessible to artists who may not have access to traditional art supplies or training.\\n\\nPublic space is also an important factor in the creation and reception of these art forms. Street art and public installation art are often created in spaces that are visible to the public, such as walls, buildings, and parks. This means that the art becomes part of the fabric of the community, contributing to the cultural landscape of the area.\\n\\nIn conclusion, contemporary street art and public installation art challenge traditional notions of art and aesthetics by embracing accessibility, non-traditional materials, and public space. These art forms engage with and reflect the cultural and social climates of the communities where they are created, addressing issues that are relevant to those communities. Factors like accessibility, public space, and the use of non-traditional materials play important roles in the creation and reception of these art forms.',\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Can you please provide more details about the impact that street art and public installation art have on the communities that they are created in?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Yes, of course! Street art and public installation art can have a significant impact on the communities where they are created, both in terms of social and cultural impact. One of the key ways in which these art forms can impact a community is by providing a platform for social and political commentary. Many street artists use their work as a means of raising awareness about important social and political issues, such as inequality, police brutality, or environmental degradation. By highlighting these issues in a visible and public way, they can draw attention to them and encourage discussion and debate. Street art and public installation art can also help to foster a sense of community and belonging. When art is created in public spaces, it can help to create a shared sense of ownership and identity amongst community members. This can be particularly important in areas that are otherwise neglected or marginalized, as it can serve as a way for those communities to reclaim and celebrate their own cultural heritage and identity. In addition, street art and public installation art can have an economic impact on the communities where they are created. Public art projects can attract tourism and investment, as well as providing opportunities for local artists and businesses. They can also contribute to the revitalization of urban areas and help to combat urban decay and blight. Overall, street art and public installation art can have a wide range of impacts on the communities where they are created, ranging from social and cultural impacts to economic ones. By bringing art out of the gallery and into public spaces, these art forms help to make art more accessible, engaging, and relevant to a wider audience.',\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Can you provide more examples of how street art and public installation art have impacted marginalized communities specifically? And also, can you explain how the use of non-traditional materials in these art forms adds a unique dimension to them?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Certainly! Street art and public installation art have had a significant impact on marginalized communities in a number of ways. One example of this is the work of the artist JR, who has created large-scale outdoor installations featuring portraits of residents in marginalized communities, such as refugees, immigrants, and prisoners. The artwork, which is often displayed on the side of buildings or other public spaces, serves as a powerful statement on the humanity of these often marginalized individuals and communities. Another example can be seen in the work of the street artist Shepard Fairey, who is known for his iconic \"Hope\" poster featuring Barack Obama during his presidential campaign. Fairey has also created numerous street art murals that address issues such as income inequality, police brutality, and gender equality. His work often features images of marginalized individuals and groups, drawing attention to their struggles and creating a sense of community around these issues. Regarding non-traditional materials, the use of these materials can add a unique dimension to these art forms. For example, street artists often use spray paint, markers, and stencils, which allows for quick and expressive work. The use of these materials also allows for a level of improvisation and experimentation that is not often seen in traditional art forms. Similarly, public installation artists often use found objects and recycled or repurposed materials in their work. This not only creates an environmentally-friendly approach to art-making but also adds a layer of meaning to the artwork. The use of found objects can also serve to reinforce the idea that art can be made from anything and that creativity is accessible to all, regardless of socioeconomic status. In conclusion, street art and public installation art have made a significant impact on marginalized communities by amplifying their voices and struggles, and by creating a sense of community around these issues. Furthermore, the use of non-traditional materials adds a unique dimension to these art forms, allowing for greater experimentation and creativity while also reinforcing the idea that art can be made from anything.',\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Can you provide some examples of public installations that have sparked community involvement and participation? Also, can you delve deeper into the social and cultural impacts that street art and public installation art can have on marginalized communities?',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs[0]\n",
    "# tokens_for_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec786f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(rows, min(5, len(rows)))\n",
    "mean_tok = np.mean([tok_fn(r) for r in sample])\n",
    "mean_tok * len(rows), mean_tok, len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset                   Avg tokens/shard       Std  N shards\n",
      "--------------------------------------------------------------\n",
      "fineweb_edu                        54.9M    1.2M        10\n",
      "smoltalk                           31.7M    1.1M        10\n",
      "ultrachat_gen                       7.8M    0.2M        10\n",
      "ultrachat_sft                       8.5M    0.2M        10\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"Dataset\":<22}  {\"Avg tokens/shard\":>18}  {\"Std\":>8}  {\"N shards\":>8}')\n",
    "print('-' * 62)\n",
    "print(f'{\"fineweb_edu\":<22}  {fw_avg/1e6:>15.1f}M  {fw_std/1e6:>5.1f}M  {N_SHARDS:>8}')\n",
    "for src in SOURCES:\n",
    "    vals = np.array(shard_totals[src])\n",
    "    avg  = vals.mean()\n",
    "    std  = vals.std()\n",
    "    n    = int((vals > 0).sum())\n",
    "    print(f'{src:<22}  {avg/1e6:>15.1f}M  {std/1e6:>5.1f}M  {n:>8}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
