{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Dataset Inspection\n",
    "\n",
    "Character counts, token counts, and pad% estimates for all training datasets.\n",
    "Includes a \"model view\" section showing exactly what token sequences the model receives,\n",
    "with special tokens visible and loss-mask annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 50262\n",
      "BOS=50257, EOS=50256, PAD=50258\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '<|beginoftext|>',\n",
    "    'pad_token': '<|pad|>',\n",
    "    'additional_special_tokens': ['<|user|>', '<|assistant|>', '<|system|>'],\n",
    "})\n",
    "print(f'Vocab: {len(tokenizer)}')\n",
    "print(f'BOS={tokenizer.bos_token_id}, EOS={tokenizer.eos_token_id}, PAD={tokenizer.pad_token_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fineweb  : 1706 shards\n",
      "conv     : 31 shards\n",
      "lima     : 1 files\n",
      "mmlu     : train=1, test=1\n",
      "gsm8k    : train=1, test=1\n"
     ]
    }
   ],
   "source": [
    "import glob as _glob_mod\n",
    "\n",
    "N_SAMPLES = 500    # rows to sample per dataset for stats\n",
    "SEQ_LEN   = 2048  # packing chunk length\n",
    "\n",
    "BASE_DIR     = os.path.join(os.getcwd(), '..', 'data')\n",
    "fineweb_dir  = os.path.join(BASE_DIR, 'base_data')\n",
    "conv_dir     = os.path.join(BASE_DIR, 'conversation_data')\n",
    "lima_dir     = os.path.join(BASE_DIR, 'lima_data')\n",
    "mmlu_dir     = os.path.join(BASE_DIR, 'mmlu_data')\n",
    "gsm8k_dir    = os.path.join(BASE_DIR, 'gsm8k_data')\n",
    "\n",
    "def _glob(d, pattern='*.parquet'):\n",
    "    if not os.path.isdir(d): return []\n",
    "    return sorted(_glob_mod.glob(os.path.join(d, pattern)))\n",
    "\n",
    "fineweb_files = _glob(fineweb_dir)\n",
    "conv_files    = _glob(conv_dir)\n",
    "lima_files    = _glob(lima_dir)\n",
    "mmlu_files    = {'train': _glob(mmlu_dir, 'train.parquet'),\n",
    "                 'test':  _glob(mmlu_dir, 'test.parquet')}\n",
    "gsm8k_files   = {'train': _glob(gsm8k_dir, 'train.parquet'),\n",
    "                 'test':  _glob(gsm8k_dir, 'test.parquet')}\n",
    "\n",
    "print(f'fineweb  : {len(fineweb_files)} shards')\n",
    "print(f'conv     : {len(conv_files)} shards')\n",
    "print(f'lima     : {len(lima_files)} files')\n",
    "print(f'mmlu     : train={len(mmlu_files[\"train\"])}, test={len(mmlu_files[\"test\"])}')\n",
    "print(f'gsm8k    : train={len(gsm8k_files[\"train\"])}, test={len(gsm8k_files[\"test\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_id = tokenizer.bos_token_id\n",
    "eos_id = tokenizer.eos_token_id\n",
    "pad_id = tokenizer.pad_token_id\n",
    "role_ids = {\n",
    "    'user':      tokenizer.convert_tokens_to_ids('<|user|>'),\n",
    "    'assistant': tokenizer.convert_tokens_to_ids('<|assistant|>'),\n",
    "    'system':    tokenizer.convert_tokens_to_ids('<|system|>'),\n",
    "}\n",
    "\n",
    "def tokenize_conv(messages):\n",
    "    \"\"\"Mirror _tokenize_conversation with mask_policy='assistant_only'.\n",
    "    Returns (token_ids, loss_mask) both of same length.\n",
    "    \"\"\"\n",
    "    tokens, mask = [bos_id], [0]\n",
    "    prev_role = None\n",
    "    for msg in messages:\n",
    "        role, content = msg['role'], msg['content']\n",
    "        rid = role_ids.get(role)\n",
    "        is_asst     = (role == 'assistant')\n",
    "        is_turn_end = (role == 'user' and prev_role == 'assistant')\n",
    "        if rid is not None:\n",
    "            tokens.append(rid)\n",
    "            mask.append(1 if (is_asst or is_turn_end) else 0)\n",
    "        ids = tokenizer.encode(content, add_special_tokens=False)\n",
    "        tokens.extend(ids)\n",
    "        mask.extend([1 if is_asst else 0] * len(ids))\n",
    "        prev_role = role\n",
    "    tokens.append(eos_id)\n",
    "    mask.append(1 if prev_role == 'assistant' else 0)\n",
    "    return tokens, mask\n",
    "\n",
    "def make_packed_chunk(msgs_iter, seq_len=SEQ_LEN):\n",
    "    \"\"\"Pack conversations (BOS-aligned) until one chunk of seq_len is full.\n",
    "    msgs_iter: iterable of message lists (each item is one conversation).\n",
    "    Returns (token_ids, loss_mask) of length seq_len.\n",
    "    \"\"\"\n",
    "    buf_t, buf_m = [], []\n",
    "    for msgs in msgs_iter:\n",
    "        toks, m = tokenize_conv(msgs)\n",
    "        toks = toks[:seq_len]; m = m[:seq_len]\n",
    "        if buf_t and len(buf_t) + len(toks) > seq_len:\n",
    "            n = seq_len - len(buf_t)\n",
    "            buf_t.extend([pad_id] * n)\n",
    "            buf_m.extend([0] * n)\n",
    "            return buf_t, buf_m\n",
    "        buf_t.extend(toks)\n",
    "        buf_m.extend(m)\n",
    "    if buf_t and len(buf_t) < seq_len:\n",
    "        n = seq_len - len(buf_t)\n",
    "        buf_t.extend([pad_id] * n)\n",
    "        buf_m.extend([0] * n)\n",
    "    return buf_t, buf_m\n",
    "\n",
    "def model_view(tokens, mask=None, label='', show_n=200):\n",
    "    \"\"\"Print a human-readable view of what the model receives for a chunk.\"\"\"\n",
    "    n      = len(tokens)\n",
    "    n_pad  = tokens.count(pad_id)\n",
    "    n_bos  = tokens.count(bos_id)\n",
    "    n_loss = sum(mask) if mask is not None else '—'\n",
    "    text   = tokenizer.decode(tokens[:show_n], skip_special_tokens=False)\n",
    "    suffix = f'  …+{n - show_n} more tokens' if n > show_n else ''\n",
    "    print(f'\\n{\"─\"*72}')\n",
    "    print(f'  {label}')\n",
    "    print(f'  {n:,} tokens | {n_bos} conv(s) | {n_pad} pad | {n_loss} loss tokens')\n",
    "    print(f'{\"─\"*72}')\n",
    "    print(f'  {text}{suffix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "Simulates the actual packing seen during training to estimate total chunks and steps per epoch.\n",
    "\n",
    "- **fineweb**: linear stream — `total_chunks = total_docs × mean_toks / SEQ_LEN` (no padding)\n",
    "- **conv datasets**: BOS-aligned packing simulated on N_SAMPLES, scaled to full dataset row count\n",
    "- Row counts read from parquet metadata (no data loaded); source fraction estimated from sampled shards for mixed-source conv files\n",
    "- **Steps** = total_chunks ÷ effective_batch (batch_size × world_size × grad_accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2523 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fineweb: 500 samples, 91,190,476 total rows\n",
      "  smoltalk: 500 samples, 1,040,783 total rows\n",
      "  ultrachat_gen: 500 samples, 255,393 total rows\n",
      "  ultrachat_sft: 500 samples, 203,313 total rows\n",
      "  lima: 500 samples, 1,030 total rows\n",
      "  mmlu_train: 500 samples, 99,842 total rows\n",
      "  mmlu_test: 500 samples, 115,700 total rows\n",
      "  gsm8k_train: 500 samples, 7,473 total rows\n",
      "  gsm8k_test: 500 samples, 1,319 total rows\n",
      "\n",
      "Batch config: 18/GPU × 4 GPUs × grad_accum 1 = 72 chunks/step\n",
      "\n",
      "Dataset               Total rows    Tok μ    Pad%    Chunks (est)     Steps @72\n",
      "───────────────────────────────────────────────────────────────────────────────\n",
      "fineweb               91,190,476     1127       —      50,193,856       697,136\n",
      "smoltalk               1,040,783      906   22.0%         541,207         7,516\n",
      "ultrachat_gen            255,393      931   24.1%         150,171         2,085\n",
      "ultrachat_sft            203,313     1213   26.1%         156,144         2,168\n",
      "lima                       1,030      704   22.7%             440             6\n",
      "mmlu_train                99,842      377   10.7%          20,567           285\n",
      "mmlu_test                115,700      339   11.9%          21,751           302\n",
      "gsm8k_train                7,473      153    4.1%             582             8\n",
      "gsm8k_test                 1,319      158    6.2%             108             1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 18\n",
    "WORLD_SIZE = 4\n",
    "GRAD_ACCUM = max(1, 4 // WORLD_SIZE)\n",
    "EFF_BATCH  = BATCH_SIZE * WORLD_SIZE * GRAD_ACCUM  # chunks per training step\n",
    "\n",
    "def get_total_rows(files, meta_sample=50):\n",
    "    \"\"\"Estimate total rows via parquet metadata — no data loaded.\"\"\"\n",
    "    if not files: return 0\n",
    "    sample = random.sample(files, min(meta_sample, len(files)))\n",
    "    mean_rows = np.mean([pq.read_metadata(f).num_rows for f in sample])\n",
    "    return int(mean_rows * len(files))\n",
    "\n",
    "def sample_rows_with_count(files, n, source_filter=None):\n",
    "    \"\"\"Sample up to n rows and estimate total rows for this (filtered) source.\n",
    "    For mixed-source files (conv_files), estimates source fraction from the sample.\n",
    "    \"\"\"\n",
    "    if not files: return [], 0\n",
    "    total_file_rows = get_total_rows(files)\n",
    "\n",
    "    pool, pool_file_rows = [], 0\n",
    "    for f in random.sample(files, min(len(files), 8)):\n",
    "        file_rows = pq.read_table(f).to_pylist()\n",
    "        pool_file_rows += len(file_rows)\n",
    "        if source_filter:\n",
    "            file_rows = [r for r in file_rows if r.get('source') == source_filter]\n",
    "        pool.extend(file_rows)\n",
    "        if len(pool) >= n * 3: break\n",
    "\n",
    "    sampled = random.sample(pool, min(n, len(pool)))\n",
    "\n",
    "    if source_filter and pool_file_rows > 0:\n",
    "        src_fraction = len(pool) / pool_file_rows\n",
    "        total_rows = int(total_file_rows * src_fraction)\n",
    "    else:\n",
    "        total_rows = total_file_rows\n",
    "\n",
    "    return sampled, total_rows\n",
    "\n",
    "def sim_pack(conv_lengths, seq_len=SEQ_LEN):\n",
    "    \"\"\"Simulate BOS-aligned packing. Returns (n_chunks, pad_pct).\"\"\"\n",
    "    buf = n_chunks = padding = total = 0\n",
    "    for L in conv_lengths:\n",
    "        L = min(L, seq_len)\n",
    "        if buf > 0 and buf + L > seq_len:\n",
    "            padding += seq_len - buf\n",
    "            total   += seq_len\n",
    "            n_chunks += 1\n",
    "            buf = L\n",
    "        else:\n",
    "            buf += L\n",
    "    if buf > 0:\n",
    "        padding += seq_len - buf\n",
    "        total   += seq_len\n",
    "        n_chunks += 1\n",
    "    return n_chunks, (padding / max(total, 1)) * 100\n",
    "\n",
    "datasets = [\n",
    "    ('fineweb',       'text', fineweb_files,         None),\n",
    "    ('smoltalk',      'conv', conv_files,             'smoltalk'),\n",
    "    ('ultrachat_gen', 'conv', conv_files,             'ultrachat_gen'),\n",
    "    ('ultrachat_sft', 'conv', conv_files,             'ultrachat_sft'),\n",
    "    ('lima',          'conv', lima_files,             None),\n",
    "    ('mmlu_train',    'conv', mmlu_files['train'],    None),\n",
    "    ('mmlu_test',     'conv', mmlu_files['test'],     None),\n",
    "    ('gsm8k_train',   'conv', gsm8k_files['train'],  None),\n",
    "    ('gsm8k_test',    'conv', gsm8k_files['test'],   None),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for name, dtype, files, src_filter in datasets:\n",
    "    rows, total_rows = sample_rows_with_count(files, N_SAMPLES, source_filter=src_filter)\n",
    "    if not rows:\n",
    "        print(f'  {name}: NOT FOUND — skipping')\n",
    "        continue\n",
    "\n",
    "    if dtype == 'text':\n",
    "        tok_counts   = [len(tokenizer.encode(r['text'], add_special_tokens=False)) + 1 for r in rows]\n",
    "        mean_toks    = np.mean(tok_counts)\n",
    "        total_chunks = int(total_rows * mean_toks / SEQ_LEN)  # linear, no padding\n",
    "        pad_pct      = None\n",
    "    else:\n",
    "        msgs_list    = [r['messages'] for r in rows]\n",
    "        tok_counts   = [len(tokenize_conv(msgs)[0]) for msgs in msgs_list]\n",
    "        mean_toks    = np.mean(tok_counts)\n",
    "        n_sample_chunks, pad_pct = sim_pack(tok_counts)\n",
    "        total_chunks = int(n_sample_chunks * (total_rows / len(rows)))\n",
    "\n",
    "    results[name] = dict(\n",
    "        total_rows=total_rows, mean_toks=mean_toks,\n",
    "        pad_pct=pad_pct, total_chunks=total_chunks,\n",
    "        steps=total_chunks // EFF_BATCH,\n",
    "    )\n",
    "    print(f'  {name}: {len(rows)} samples, {total_rows:,} total rows')\n",
    "\n",
    "print(f'\\nBatch config: {BATCH_SIZE}/GPU × {WORLD_SIZE} GPUs × grad_accum {GRAD_ACCUM} = {EFF_BATCH} chunks/step\\n')\n",
    "hdr = f'{\"Dataset\":<18}  {\"Total rows\":>12}  {\"Tok μ\":>7}  {\"Pad%\":>6}  {\"Chunks (est)\":>14}  {\"Steps @\"+str(EFF_BATCH):>12}'\n",
    "print(hdr)\n",
    "print('─' * len(hdr))\n",
    "for name, s in results.items():\n",
    "    pad = f'{s[\"pad_pct\"]:.1f}%' if s['pad_pct'] is not None else '    —'\n",
    "    print(f'{name:<18}  {s[\"total_rows\"]:>12,}  {s[\"mean_toks\"]:>7.0f}  {pad:>6}  '\n",
    "          f'{s[\"total_chunks\"]:>14,}  {s[\"steps\"]:>12,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Model View — what the model sees\n",
    "\n",
    "Each block shows the actual decoded token stream the model receives, with special tokens visible.  \n",
    "Header line shows total tokens, number of packed conversations, pad count, and loss-token count.\n",
    "\n",
    "- **fineweb**: linear stream (no BOS, `<|endoftext|>` between docs, all tokens trained)\n",
    "- **conv datasets**: BOS-aligned, `<|user|>` / `<|assistant|>` role tokens, loss only on assistant spans\n",
    "- **mmlu_train**: single packed chunk showing multiple MC questions back-to-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "────────────────────────────────────────────────────────────────────────\n",
      "  fineweb_edu  [linear packed — no BOS, <|endoftext|> between docs]\n",
      "  2,048 tokens | 0 conv(s) | 0 pad | — loss tokens\n",
      "────────────────────────────────────────────────────────────────────────\n",
      "  Five foods that can stain your teeth\n",
      "- 6 Months ago\n",
      "Determined to keep your teeth shining bright? You already know how important it is to brush and floss daily, and to avoid smoking or chewing tobacco—and see a dentist periodically. However, dentists also suggest you to be mindful of certain foods and beverages to prevent your teeth from staining. Here is a list of 5 such foods and beverages that will stain your teeth.\n",
      "- Black Coffee-The darker the beverage, more the staining, this is because the outer layer of the tooth is very porous and absorbs the dark colored beverage causing the teeth to stain. By that, I don't mean to say one must stop drinking coffee altogether, but you can make a lighter coffee, add some milk to lighten it and do away with the harm. Milk will also help you with some amount of calcium and vitamin D, which will keep your teeth and bones strong.\n",
      "- Tea- Tea contains tannins which stain  …+1848 more tokens\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────\n",
      "  smoltalk  [3 turns, 267 tokens]\n",
      "  267 tokens | 1 conv(s) | 0 pad | 159 loss tokens\n",
      "────────────────────────────────────────────────────────────────────────\n",
      "  <|beginoftext|><|system|>You're an AI assistant for text re-writing. Rewrite the input text to make it more friendly and approachable while maintaining its main points.<|user|>Mike,\n",
      "\n",
      "I've reviewed your proposed changes to the curriculum, and I have to say, I'm disappointed. Your approach is outdated and ineffective. We need to focus on evidence-based methods that actually help our students, not just what you think is engaging.\n",
      "\n",
      "Thanks for making my job harder, Sarah.\n",
      "\n",
      "Sarah Thompson\n",
      "Middle School Health Educator<|assistant|>Hi Mike,\n",
      "\n",
      "I hope you're doing well! I’ve had a chance to look over the changes you suggested for the curriculum, and I wanted to share some thoughts.\n",
      "\n",
      "While I appreciate your effort to make the content more engaging, I think we might need to consider a more evidence-based approach. Our goal is to ensure that the methods we use are not only engaging but also effective in helping our students learn and grow.  …+67 more tokens\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────\n",
      "  lima  [2 turns, 705 tokens]\n",
      "  705 tokens | 1 conv(s) | 0 pad | 555 loss tokens\n",
      "────────────────────────────────────────────────────────────────────────\n",
      "  <|beginoftext|><|user|>\n",
      "  Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof; or abridging the freedom of speech, or of the press; or the right of the people peaceably to assemble, and to petition the Government for a redress of grievances.\n",
      "\n",
      "I don't understand the part marked in bold.\n",
      "Congress shall make no law prohibiting the free exercise of religion. So congress should not make a law which prohibits the freedom of religion. I get it.\n",
      "But Congress shall make a law which respects an establishment of religion. Doesn't \"Congress shall make no law respecting an establishment of religion\" mean congress should not make a law that respects religion because there's \"no\" in it??  <|assistant|>The confusing term seems to be \"respecting\". This is a somewhat different meaning of \"respect\", that is still in common use:\n",
      "\n",
      "  respecting (prep): \n",
      "  \n",
      "  * in view of : considering\n",
      "  * with  …+505 more tokens\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────\n",
      "  mmlu_train  [packed 2048-token chunk, multiple MC questions]\n",
      "  2,048 tokens | 5 conv(s) | 336 pad | 35 loss tokens\n",
      "────────────────────────────────────────────────────────────────────────\n",
      "  <|beginoftext|><|user|>Multiple Choice question: Dear Sir, Good day! I want to become a pilot  , because my cousin is a pilot . I think he is great. He is cool when he wears his pilot uniform. If I become a pilot , my parents will be proud  of me and I can travel around the world . I am in the third grade in a university. I'm tall with medium build; I'm quite healthy and strong. I like playing sports .If I have a chance to become a pilot, I will have a great time flying in the blue sky. I believe I can be a good pilot .The problem is that I can't  afford  my dream .You know, to be a pilot needs a lot of money. My parents are both workers. They don't have much money. Could you help me find a sponsor ( ),sir? And how can I get a scholarship  ? I am _ your help, sir? If you have time, you can e-mail me at www,cn,com . Your help will mean a lot, sir. Once again good day and thank you. Yours   Wei What's Wei's problem ?\n",
      "- He isn't tall enough.=A\n",
      "- He is a university student.=B\n",
      "- He doesn't have much money.=C\n",
      "- His parents can't afford his dreams.=D<|assistant|>He doesn't have much money.<|user|><|endoftext|><|beginoftext|><|user|>Multiple Choice question: Born in  …+1748 more tokens\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────\n",
      "  gsm8k_train  [132 tokens]\n",
      "  132 tokens | 1 conv(s) | 0 pad | 78 loss tokens\n",
      "────────────────────────────────────────────────────────────────────────\n",
      "  <|beginoftext|><|user|>Tom needs to lower a rope down 6 stories.   One story is 10 feet.  The only rope being sold is 20 feet long but you lose 25% when lashing them together.  How many pieces of rope will he need to buy?<|assistant|>He needs 10*6=<<10*6=60>>60 feet\n",
      "He loses 20*.25=<<20*.25=5>>5 feet each time\n",
      "So he gets 20-5=<<20-5=15>>15 feet from each piece\n",
      "That means he needs 60/15=<<60/15=4>>4 pieces of rope\n",
      "#### 4<|user|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "# ── FineWeb-Edu (linear packed stream — no BOS, EOS between docs) ─────────────\n",
    "if fineweb_files:\n",
    "    texts = pq.read_table(random.choice(fineweb_files))['text'].to_pylist()\n",
    "    stream = []\n",
    "    for t in random.sample(texts, min(20, len(texts))):\n",
    "        stream.extend(tokenizer.encode(t, add_special_tokens=False))\n",
    "        stream.append(eos_id)\n",
    "        if len(stream) >= SEQ_LEN: break\n",
    "    model_view(stream[:SEQ_LEN],\n",
    "               label='fineweb_edu  [linear packed — no BOS, <|endoftext|> between docs]',\n",
    "               show_n=200)\n",
    "else:\n",
    "    print('fineweb: not found')\n",
    "\n",
    "# ── SmolTalk (multi-turn conv) ────────────────────────────────────────────────\n",
    "if conv_files:\n",
    "    pool = []\n",
    "    for f in random.sample(conv_files, min(3, len(conv_files))):\n",
    "        pool += [r for r in pq.read_table(f).to_pylist() if r['source'] == 'smoltalk']\n",
    "    if pool:\n",
    "        ex = random.choice(pool)\n",
    "        toks, mask = tokenize_conv(ex['messages'])\n",
    "        model_view(toks, mask,\n",
    "                   label=f'smoltalk  [{len(ex[\"messages\"])} turns, {len(toks)} tokens]',\n",
    "                   show_n=200)\n",
    "\n",
    "# ── LIMA (conv, longer outputs) ───────────────────────────────────────────────\n",
    "if lima_files:\n",
    "    rows = pq.read_table(lima_files[0]).to_pylist()\n",
    "    ex = random.choice(rows)\n",
    "    toks, mask = tokenize_conv(ex['messages'])\n",
    "    model_view(toks, mask,\n",
    "               label=f'lima  [{len(ex[\"messages\"])} turns, {len(toks)} tokens]',\n",
    "               show_n=200)\n",
    "else:\n",
    "    print('lima: not found')\n",
    "\n",
    "# ── MMLU train (packed chunk — multiple MC questions per 2048-token window) ───\n",
    "if mmlu_files['train']:\n",
    "    rows = pq.read_table(mmlu_files['train'][0]).to_pylist()\n",
    "    random.shuffle(rows)\n",
    "    chunk_toks, chunk_mask = make_packed_chunk(r['messages'] for r in rows)\n",
    "    model_view(chunk_toks, chunk_mask,\n",
    "               label=f'mmlu_train  [packed {SEQ_LEN}-token chunk, multiple MC questions]',\n",
    "               show_n=300)\n",
    "else:\n",
    "    print('mmlu: not found — run: python -m core.dataset --mmlu')\n",
    "\n",
    "# ── GSM8K train (single example, step-by-step reasoning) ─────────────────────\n",
    "if gsm8k_files['train']:\n",
    "    rows = pq.read_table(gsm8k_files['train'][0]).to_pylist()\n",
    "    ex = random.choice(rows)\n",
    "    toks, mask = tokenize_conv(ex['messages'])\n",
    "    model_view(toks, mask,\n",
    "               label=f'gsm8k_train  [{len(toks)} tokens]',\n",
    "               show_n=200)\n",
    "else:\n",
    "    print('gsm8k: not found — run: python -m core.dataset --gsm8k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
